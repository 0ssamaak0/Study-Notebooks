{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video link](https://www.youtube.com/watch?v=0VdNflU08yA&t=17s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assymmetric VS Symmetric Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a simple tensor to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.59776399 -48.74568247   0.         146.59776399 121.51449439\n",
      "  96.0871144   74.92129314 -28.3979833  128.46395637 106.17554416\n",
      " 142.6507787  -47.05849674 141.70347562 -11.01861359 123.20054291\n",
      " -40.89760992  84.60083393 139.80503778   2.6165592  -47.74568247]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# suppress scientific notation and round to 2 decimal places\n",
    "np.set_printoptions(suppress=2)\n",
    "\n",
    "# Generate uniform distribution\n",
    "params = np.random.uniform(low=-50, high=150, size=20)\n",
    "\n",
    "# make important value at the beginning (for better debugging)\n",
    "params[0] = params.max() + 1\n",
    "params[1] = params.min() - 1\n",
    "params[2] = 0\n",
    "\n",
    "\n",
    "# print\n",
    "print(params)\n",
    "\n",
    "video_params = np.array(\n",
    "    [43.31, -44.93, 0, 22.99, -43.93, -11.35, 38.48, -20.49, -38.61, -28.02]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(arr, lower_bound, upper_bound):\n",
    "    arr[arr < lower_bound] = lower_bound\n",
    "    arr[arr > upper_bound] = upper_bound\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assymmetric_quantization(params, nbits):\n",
    "    alpha = np.max(params)\n",
    "    beta = np.min(params)\n",
    "\n",
    "    scale = (alpha - beta) / ((2**nbits) - 1)\n",
    "    zero = -1 * np.round(beta / scale)\n",
    "\n",
    "    quantized = clamp(np.round(params / scale + zero), 0, (2**nbits) - 1).astype(\n",
    "        np.int32\n",
    "    )\n",
    "\n",
    "    return quantized, scale, zero\n",
    "\n",
    "\n",
    "def assymmetric_dequantization(qparams, scale, zero):\n",
    "    return scale * (qparams - zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original params [147.59776399 -48.74568247   0.         146.59776399 121.51449439\n",
      "  96.0871144   74.92129314 -28.3979833  128.46395637 106.17554416\n",
      " 142.6507787  -47.05849674 141.70347562 -11.01861359 123.20054291\n",
      " -40.89760992  84.60083393 139.80503778   2.6165592  -47.74568247]\n",
      "Quantized params [255   0  63 253 221 188 160  26 230 201 248   2 247  49 223  10 173 245\n",
      "  66   1]\n",
      "dequantized params [147.83506557 -48.50838089   0.         146.29511697 121.65593937\n",
      "  96.24678748  74.68750708 -28.48904909 128.58570807 106.25645338\n",
      " 142.44524547 -46.96843229 141.67527117 -10.7796402  123.19588797\n",
      " -40.80863789  84.69717298 140.13532257   2.3099229  -47.73840659]\n",
      "MSE: 0.03313425676780694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original params {params}\")\n",
    "quantized, scale, zero = assymmetric_quantization(params, 8)\n",
    "print(f\"Quantized params {quantized}\")\n",
    "dequantized = assymmetric_dequantization(quantized, scale, zero)\n",
    "print(f\"dequantized params {dequantized}\")\n",
    "print(f\"MSE: {np.mean((params - dequantized) ** 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_quantization(params, nbits):\n",
    "    alpha = np.max(np.abs(params))\n",
    "    scale = alpha / ((2 ** (nbits - 1)) - 1)\n",
    "\n",
    "    lower_bound = -1 * (2 ** (nbits - 1))\n",
    "    upper_bound = (2 ** (nbits - 1)) - 1\n",
    "\n",
    "    quantized = clamp(np.round(params / scale), lower_bound, upper_bound).astype(\n",
    "        np.int32\n",
    "    )\n",
    "    return quantized, scale\n",
    "\n",
    "\n",
    "def symmetric_dequantization(qparams, scale):\n",
    "    return scale * qparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original params [147.59776399 -48.74568247   0.         146.59776399 121.51449439\n",
      "  96.0871144   74.92129314 -28.3979833  128.46395637 106.17554416\n",
      " 142.6507787  -47.05849674 141.70347562 -11.01861359 123.20054291\n",
      " -40.89760992  84.60083393 139.80503778   2.6165592  -47.74568247]\n",
      "Quantized params [127 -42   0 126 105  83  64 -24 111  91 123 -40 122  -9 106 -35  73 120\n",
      "   2 -41]\n",
      "dequantized params [147.59776399 -48.81185896   0.         146.43557687 122.02964739\n",
      "  96.4615308   74.37997555 -27.89249083 129.0027701  105.75902774\n",
      " 142.94901552 -46.48748472 141.7868284  -10.45968406 123.19183451\n",
      " -40.67654913  84.83965962 139.46245416   2.32437424 -47.64967184]\n",
      "MSE: 0.12504463234472618\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original params {params}\")\n",
    "quantized, scale = symmetric_quantization(params, 8)\n",
    "print(f\"Quantized params {quantized}\")\n",
    "dequantized = symmetric_dequantization(quantized, scale)\n",
    "print(f\"dequantized params {dequantized}\")\n",
    "print(f\"MSE: {np.mean((params - dequantized) ** 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Alpha and Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uniform distribution\n",
    "params = np.random.uniform(low=-50, high=150, size=10000)\n",
    "\n",
    "# make important value at the beginning (for better debugging)\n",
    "params[0] = params.max() + 1\n",
    "params[1] = params.min() - 1\n",
    "params[2] = 0\n",
    "\n",
    "# set an outlier\n",
    "params[-1] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assymmetric_quantization_percentile(params, nbits, percentile=99.99):\n",
    "    alpha = np.percentile(params, percentile)\n",
    "    beta = np.percentile(params, 100 - percentile)\n",
    "\n",
    "    scale = (alpha - beta) / ((2**nbits) - 1)\n",
    "    zero = -1 * np.round(beta / scale)\n",
    "\n",
    "    quantized = clamp(np.round(params / scale + zero), 0, (2**nbits) - 1).astype(\n",
    "        np.int32\n",
    "    )\n",
    "\n",
    "    return quantized, scale, zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values: [ 150.99113313  -50.97723924    0.         ...   47.71223358  -46.29745736\n",
      " 1000.        ]\n",
      "dequantized minmax: [ 152.49473667  -49.45775243    0.         ...   49.45775243  -45.33627307\n",
      " 1001.5194868 ]\n",
      "dequantized percentile: [151.38136339 -49.67200986   0.         ...  48.09512066 -46.51823146\n",
      " 151.38136339]\n",
      "MSE: 1.423691734308575\n",
      "MSE: 72.06796435785438\n",
      "MSE (no outlier): 1.4236032106145997\n",
      "MSE (no outlier): 0.052610579512310615\n"
     ]
    }
   ],
   "source": [
    "quantized_minmax, scale_minmax, zero_minmax = assymmetric_quantization(params, 8)\n",
    "dequantized_minmax = assymmetric_dequantization(\n",
    "    quantized_minmax, scale_minmax, zero_minmax\n",
    ")\n",
    "\n",
    "quantized_percentile, scale_percentile, zero_percentile = (\n",
    "    assymmetric_quantization_percentile(params, 8)\n",
    ")\n",
    "dequantized_percentile = assymmetric_dequantization(\n",
    "    quantized_percentile, scale_percentile, zero_percentile\n",
    ")\n",
    "\n",
    "# the outlier suffers from big quantization error in percentile\n",
    "print(f\"Original values: {params}\")\n",
    "print(f\"dequantized minmax: {dequantized_minmax}\")\n",
    "print(f\"dequantized percentile: {dequantized_percentile}\")\n",
    "\n",
    "# So MSE is larger in percentile\n",
    "print(f\"MSE: {np.mean((params - dequantized_minmax) ** 2)}\")\n",
    "print(f\"MSE: {np.mean((params - dequantized_percentile) ** 2)}\")\n",
    "\n",
    "# MSE execluding outlier\n",
    "print(f\"MSE (no outlier): {np.mean((params[:-1] - dequantized_minmax[:-1]) ** 2)}\")\n",
    "print(f\"MSE (no outlier): {np.mean((params[:-1] - dequantized_percentile[:-1]) ** 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make torch deterministic\n",
    "torch.manual_seed(0)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "# load mnist dataset\n",
    "mnist_train = datasets.MNIST(\n",
    "    root=\"./data/\", train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=mnist_train, batch_size=10, shuffle=True\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=\"./data/\", train=False, transform=transform, download=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=mnist_test, batch_size=10, shuffle=True\n",
    ")\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture, Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerySimplenet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(28 * 28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VerySimplenet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    total_iters = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            loss_sum = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "            if total_iters % 1000 == 0:\n",
    "                print(f\"Epoch: {epoch}, Iteration: {total_iters}, Loss: {loss_sum}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1013it [00:04, 169.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 1000, Loss: 0.9822236895561218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2029it [00:12, 152.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 2000, Loss: 0.33656615018844604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3016it [00:21, 132.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 3000, Loss: 0.6857504844665527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4020it [00:28, 157.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 4000, Loss: 0.03442932292819023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5019it [00:35, 190.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 5000, Loss: 0.14028537273406982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [00:40, 147.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 6000, Loss: 0.01198307704180479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = train(net, train_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Model size (Before Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    torch.save(model.state_dict(), \"model.pt\")\n",
    "    print(f\"Model size: {os.path.getsize('model.pt') / 1024:.2f} KB\")\n",
    "    os.remove(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "Parameter containing:\n",
      "tensor([[-0.0127,  0.0067, -0.0419,  ...,  0.0095, -0.0087, -0.0104],\n",
      "        [-0.0197, -0.0149, -0.0104,  ..., -0.0202, -0.0059, -0.0299],\n",
      "        [ 0.0199,  0.0550,  0.0068,  ...,  0.0197,  0.0413,  0.0481],\n",
      "        ...,\n",
      "        [ 0.0278,  0.0316, -0.0031,  ..., -0.0084,  0.0108, -0.0261],\n",
      "        [ 0.0056,  0.0137,  0.0458,  ...,  0.0261,  0.0261,  0.0256],\n",
      "        [ 0.0102,  0.0049, -0.0093,  ...,  0.0271, -0.0221, -0.0020]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights before quantization\")\n",
    "print(net.linear1.weight)\n",
    "print(net.linear1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size before quantization\n",
      "Model size: 352.36 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model size before quantization\")\n",
    "print_model_size(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Before Quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 287.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Before Quantization\")\n",
    "test(net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert MinMax Observers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedVerySimplenet(nn.Module):\n",
    "    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n",
    "        super().__init__()\n",
    "        self.quant = (\n",
    "            torch.quantization.QuantStub()\n",
    "        )  # Quantization function (will be replaced by quantization)\n",
    "        self.linear1 = nn.Linear(28 * 28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = (\n",
    "            torch.quantization.DeQuantStub()\n",
    "        )  # Dequantization function (will be replaced by dequantization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimplenet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized = QuantizedVerySimplenet().to(device)\n",
    "# Copy Weights from original model to quantized model\n",
    "net_quantized.load_state_dict(net.state_dict())\n",
    "net_quantized.eval()\n",
    "\n",
    "net_quantized.qconfig = torch.quantization.default_qconfig\n",
    "net_quantized = torch.quantization.prepare(net_quantized)  # Insert observers\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate the model using Test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 277.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(net_quantized, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimplenet(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-53.42708206176758, max_val=38.368324279785156)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=100, out_features=100, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-29.53961753845215, max_val=25.241788864135742)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-25.761402130126953, max_val=22.6112003326416)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Check statistics of the various layers\")\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model using statistics collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedVerySimplenet(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.7227985262870789, zero_point=74, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.43134966492652893, zero_point=68, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.38088664412498474, zero_point=68, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized = torch.quantization.convert(net_quantized)  # Convert to quantized model\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Model size (After Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[-3,  1, -9,  ...,  2, -2, -2],\n",
      "        [-4, -3, -2,  ..., -4, -1, -6],\n",
      "        [ 4, 11,  1,  ...,  4,  8, 10],\n",
      "        ...,\n",
      "        [ 6,  6, -1,  ..., -2,  2, -5],\n",
      "        [ 1,  3,  9,  ...,  5,  5,  5],\n",
      "        [ 2,  1, -2,  ...,  6, -5,  0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights after quantization\")\n",
    "print(torch.int_repr(net_quantized.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights\n",
      "Parameter containing:\n",
      "tensor([[-0.0127,  0.0067, -0.0419,  ...,  0.0095, -0.0087, -0.0104],\n",
      "        [-0.0197, -0.0149, -0.0104,  ..., -0.0202, -0.0059, -0.0299],\n",
      "        [ 0.0199,  0.0550,  0.0068,  ...,  0.0197,  0.0413,  0.0481],\n",
      "        ...,\n",
      "        [ 0.0278,  0.0316, -0.0031,  ..., -0.0084,  0.0108, -0.0261],\n",
      "        [ 0.0056,  0.0137,  0.0458,  ...,  0.0261,  0.0261,  0.0256],\n",
      "        [ 0.0102,  0.0049, -0.0093,  ...,  0.0271, -0.0221, -0.0020]],\n",
      "       requires_grad=True)\n",
      "Dequantized weights\n",
      "tensor([[-0.0146,  0.0049, -0.0437,  ...,  0.0097, -0.0097, -0.0097],\n",
      "        [-0.0194, -0.0146, -0.0097,  ..., -0.0194, -0.0049, -0.0291],\n",
      "        [ 0.0194,  0.0534,  0.0049,  ...,  0.0194,  0.0388,  0.0486],\n",
      "        ...,\n",
      "        [ 0.0291,  0.0291, -0.0049,  ..., -0.0097,  0.0097, -0.0243],\n",
      "        [ 0.0049,  0.0146,  0.0437,  ...,  0.0243,  0.0243,  0.0243],\n",
      "        [ 0.0097,  0.0049, -0.0097,  ...,  0.0291, -0.0243,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# compare original weights and dequantized weights\n",
    "print(\"Original weights\")\n",
    "print(net.linear1.weight)\n",
    "print(\"Dequantized weights\")\n",
    "print(torch.dequantize(net_quantized.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after quantization\n",
      "Model size: 92.95 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model size after quantization\")\n",
    "print_model_size(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Aefore Quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 272.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Aefore Quantization\")\n",
    "test(net_quantized, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
