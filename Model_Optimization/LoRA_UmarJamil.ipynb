{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "[Video Link](https://www.youtube.com/watch?v=PXWYUTMt-AU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: torch.Size([10, 10])\n",
      "rank of W: 2\n"
     ]
    }
   ],
   "source": [
    "# Create a low rank matrix\n",
    "d, k = 10, 10\n",
    "w_rank = 2\n",
    "\n",
    "W = torch.rand(d, w_rank) @ torch.rand(w_rank, k)\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"rank of W: {np.linalg.matrix_rank(W)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of U: torch.Size([10, 10]) | Shape of S: torch.Size([10]) | Shape of V: torch.Size([10, 10])\n",
      "Shape of B: torch.Size([10, 2])\n",
      "Shape of A: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Apply SVD\n",
    "U, S, V = torch.svd(W)\n",
    "print(f\"Shape of U: {U.shape} | Shape of S: {S.shape} | Shape of V: {V.shape}\")\n",
    "\n",
    "U_r = U[:, :w_rank]\n",
    "S_r = torch.diag(S[:w_rank])\n",
    "V_r = V[:, :w_rank].T\n",
    "\n",
    "B = U_r @ S_r\n",
    "A = V_r\n",
    "\n",
    "print(f\"Shape of B: {B.shape}\")\n",
    "print(f\"Shape of A: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([2.2722, 1.7504, 2.8158, 2.1827, 2.3436, 1.9618, 1.6580, 2.9906, 1.9360,\n",
      "        2.2995])\n",
      "y_prime: tensor([2.2722, 1.7504, 2.8158, 2.1827, 2.3436, 1.9618, 1.6580, 2.9906, 1.9360,\n",
      "        2.2995])\n",
      "Difference: -4.649162292480469e-06\n",
      "Total number of params in W: 100\n",
      "Total number of params in A & B: 40\n"
     ]
    }
   ],
   "source": [
    "# For the same input, check the results using both the original and the low rank matrices\n",
    "bias = torch.rand(d)\n",
    "x = torch.rand(k)\n",
    "\n",
    "y = W @ x + bias\n",
    "y_prime = B @ A @ x + bias\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "print(f\"y_prime: {y_prime}\")\n",
    "print(f\"Difference: {torch.sum(y - y_prime)}\")\n",
    "\n",
    "print(f\"Total number of params in W: {W.numel()}\")\n",
    "print(f\"Total number of params in A & B: {A.numel() + B.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "seed = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unnecessarily large model\n",
    "class UnnecessarilyLargeModel(nn.Module):\n",
    "    def __init__(self, hidden1 = 1000, hidden2 = 2000):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden1)\n",
    "        self.linear2 = nn.Linear(hidden1, hidden2)\n",
    "        self.linear3 = nn.Linear(hidden2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "model = UnnecessarilyLargeModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [00:06<00:00, 139.27it/s, loss=0.194]\n"
     ]
    }
   ],
   "source": [
    "# train for a single epoch\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
    "\n",
    "for i, (data, target) in enumerate(progress_bar):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    progress_bar.set_postfix({'loss': total_loss/(i+1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {name: param.clone().detach() for name, param in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.967\n",
      "wrong counts for the digit 0: 13\n",
      "wrong counts for the digit 1: 15\n",
      "wrong counts for the digit 2: 31\n",
      "wrong counts for the digit 3: 50\n",
      "wrong counts for the digit 4: 22\n",
      "wrong counts for the digit 5: 14\n",
      "wrong counts for the digit 6: 23\n",
      "wrong counts for the digit 7: 49\n",
      "wrong counts for the digit 8: 51\n",
      "wrong counts for the digit 9: 58\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(x.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            else:\n",
    "                wrong_counts[y[idx]] +=1\n",
    "            total +=1\n",
    "print(f'Accuracy: {round(correct/total, 3)}')\n",
    "for i in range(len(wrong_counts)):\n",
    "    print(f'wrong counts for the digit {i}: {wrong_counts[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank = 1, alpha = 1, device = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, features_out, device=device))\n",
    "        self.lora_B = nn.Parameter(torch.randn(features_in, rank, device=device))\n",
    "        # In the paper A is initialized with a normal distribution\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            return original_weights + (self.lora_B @ self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameterization to the model (https://pytorch.org/tutorials/intermediate/parametrizations.html)\n",
    "# Instead of accessing the weights, it is now to access this function \n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "torch.nn.utils.parametrize.register_parametrization(\n",
    "    model.linear1, \"weight\", linear_layer_parameterization(model.linear1, device)\n",
    ")\n",
    "torch.nn.utils.parametrize.register_parametrization(\n",
    "    model.linear2, \"weight\", linear_layer_parameterization(model.linear2, device)\n",
    ")\n",
    "torch.nn.utils.parametrize.register_parametrization(\n",
    "    model.linear3, \"weight\", linear_layer_parameterization(model.linear3, device)\n",
    ")\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.linear1, model.linear2, model.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the original model: 2,813,804\n",
      "Total number of parameters in the LoRA model: 6,794\n",
      "Percentage of parameters in the LoRA model: 0.24%\n"
     ]
    }
   ],
   "source": [
    "# get total number of parameters (original model)\n",
    "print(f\"Total number of parameters in the original model: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "# get total number of parameters (LoRA model)\n",
    "total_parameters_lora = sum([layer.parametrizations['weight'][0].lora_A.numel() + layer.parametrizations['weight'][0].lora_B.numel() for layer in [model.linear1, model.linear2, model.linear3]])\n",
    "print(f\"Total number of parameters in the LoRA model: {total_parameters_lora:,}\")\n",
    "# percentage\n",
    "print(f\"Percentage of parameters in the LoRA model: {total_parameters_lora / sum(p.numel() for p in model.parameters()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 101/595 [00:00<00:03, 135.06it/s, loss=1.26]\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again, by keeping only the digit 9\n",
    "exclude_indices = train_dataset.targets == 9\n",
    "train_dataset.data = train_dataset.data[exclude_indices]\n",
    "train_dataset.targets = train_dataset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# train for a single epoch\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
    "\n",
    "for i, (data, target) in enumerate(progress_bar):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    progress_bar.set_postfix({'loss': total_loss/(i+1)})\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the frozen parameters are still unchanged by the fimodeluning\n",
    "assert torch.all(model.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(model.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(model.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
    "\n",
    "assert torch.equal(model.linear1.weight, model.linear1.parametrizations.weight.original + (model.linear1.parametrizations.weight[0].lora_B @ model.linear1.parametrizations.weight[0].lora_A) * model.linear1.parametrizations.weight[0].scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.103\n",
      "wrong counts for the digit 0: 980\n",
      "wrong counts for the digit 1: 1131\n",
      "wrong counts for the digit 2: 1032\n",
      "wrong counts for the digit 3: 1009\n",
      "wrong counts for the digit 4: 981\n",
      "wrong counts for the digit 5: 891\n",
      "wrong counts for the digit 6: 957\n",
      "wrong counts for the digit 7: 1016\n",
      "wrong counts for the digit 8: 973\n",
      "wrong counts for the digit 9: 0\n"
     ]
    }
   ],
   "source": [
    "# Test with LoRA\n",
    "enable_disable_lora(enabled=True)\n",
    "correct = 0\n",
    "total = 0\n",
    "wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(x.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            else:\n",
    "                wrong_counts[y[idx]] +=1\n",
    "            total +=1\n",
    "print(f'Accuracy: {round(correct/total, 3)}')\n",
    "for i in range(len(wrong_counts)):\n",
    "    print(f'wrong counts for the digit {i}: {wrong_counts[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.967\n",
      "wrong counts for the digit 0: 13\n",
      "wrong counts for the digit 1: 15\n",
      "wrong counts for the digit 2: 31\n",
      "wrong counts for the digit 3: 50\n",
      "wrong counts for the digit 4: 22\n",
      "wrong counts for the digit 5: 14\n",
      "wrong counts for the digit 6: 23\n",
      "wrong counts for the digit 7: 49\n",
      "wrong counts for the digit 8: 51\n",
      "wrong counts for the digit 9: 58\n"
     ]
    }
   ],
   "source": [
    "# Test without LoRA\n",
    "enable_disable_lora(enabled=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(x.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            else:\n",
    "                wrong_counts[y[idx]] +=1\n",
    "            total +=1\n",
    "print(f'Accuracy: {round(correct/total, 3)}')\n",
    "for i in range(len(wrong_counts)):\n",
    "    print(f'wrong counts for the digit {i}: {wrong_counts[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
