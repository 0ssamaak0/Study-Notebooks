{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9431e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted to to to to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f947b60",
   "metadata": {},
   "source": [
    "# Using Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d4f9e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.5167456e-05 -1.3562296e-04] [0.03608433 0.03608414]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.mean(1), embeddings.std(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9830327",
   "metadata": {},
   "source": [
    "# Using Transformers (We use only last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bc4e8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "last_hidden_state size: torch.Size([2, 10, 768])\n",
      "pooler_output size: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "print(model_output.keys())\n",
    "print(f\"last_hidden_state size: {model_output.last_hidden_state.shape}\")\n",
    "print(f\"pooler_output size: {model_output.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93968fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embeddings (or last_hidden_state) shape: torch.Size([2, 10, 768])\n",
      "attention mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "attention mask shape: torch.Size([2, 10])\n",
      "unsqueezed attention mask shape: torch.Size([2, 10, 1])\n",
      "expanded token embeddings shape: torch.Size([2, 10, 768])\n",
      "input_mask_expanded[0][0] should be all ones: 0\n",
      "But input_mask_expanded[0][9] should be all zeros: 0\n",
      "But input_mask_expanded[1][9] should be all ones: 768\n",
      "summed embeddings shape: torch.Size([2, 768])\n",
      "num_real_tokens shape: torch.Size([2, 768])\n",
      "mean_pooled_embeddings shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Now we need to convert the embeddings to sentence embeddings\n",
    "# We need the last_hidden_state along with the attention mask\n",
    "token_embeddings = model_output[0]\n",
    "print(f\"token embeddings (or last_hidden_state) shape: {token_embeddings.shape}\")\n",
    "\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "print(f\"attention mask:\\n{attention_mask}\")\n",
    "print(f\"attention mask shape: {attention_mask.shape}\")\n",
    "# We need to expand the attention mask\n",
    "unsqueezed_attention_mask = attention_mask.unsqueeze(-1)\n",
    "print(f\"unsqueezed attention mask shape: {unsqueezed_attention_mask.shape}\")\n",
    "# Now expand the token embeddings into the same shape of the token embeddings\n",
    "input_mask_expanded = unsqueezed_attention_mask.expand(token_embeddings.shape)\n",
    "print(f\"expanded token embeddings shape: {input_mask_expanded.shape}\")\n",
    "print(f\"input_mask_expanded[0][0] should be all ones: {sum(input_mask_expanded[0][9])}\")\n",
    "print(\n",
    "    f\"But input_mask_expanded[0][9] should be all zeros: {sum(input_mask_expanded[0][9])}\"\n",
    ")\n",
    "print(\n",
    "    f\"But input_mask_expanded[1][9] should be all ones: {sum(input_mask_expanded[1][9])}\"\n",
    ")\n",
    "# Now sum the embeddings over dim 1 (mask them by attention mask)\n",
    "summed_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "print(f\"summed embeddings shape: {summed_embeddings.shape}\")\n",
    "\n",
    "# Compute mean by dividing by the number of tokens (real tokens not padded)\n",
    "num_real_tokens = torch.clamp(\n",
    "    input_mask_expanded.sum(1), min=1e-9\n",
    ")  # We used torch.clamp to avoid division by zero\n",
    "print(f\"num_real_tokens shape: {num_real_tokens.shape}\")\n",
    "\n",
    "# Divide\n",
    "mean_pooled_embeddings = summed_embeddings / num_real_tokens\n",
    "print(f\"mean_pooled_embeddings shape: {mean_pooled_embeddings.shape}\")\n",
    "\n",
    "# L2 Normalization\n",
    "normalized_embeddings = F.normalize(mean_pooled_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b512226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.5168e-05, -1.3562e-04]) tensor([0.0361, 0.0361])\n"
     ]
    }
   ],
   "source": [
    "print(normalized_embeddings.mean(1), normalized_embeddings.std(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24fade8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8db8b6",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe73dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample_arr: torch.Size([2, 4])\n",
      "L2 Norm: tensor([[0.1826, 0.3651, 0.5477, 0.7303],\n",
      "        [0.3563, 0.4454, 0.5345, 0.6236]])\n",
      "L2 Norm itself: tensor([ 5.4772, 11.2250])\n",
      "Divided by L2 Norm: tensor([[0.1826, 0.3651, 0.5477, 0.7303],\n",
      "        [0.3563, 0.4454, 0.5345, 0.6236]])\n",
      "L2 Norm manually: tensor([ 5.4772, 11.2250])\n",
      "Divided by L2 Norm manually:  tensor([[0.1826, 0.3651, 0.5477, 0.7303],\n",
      "        [0.3563, 0.4454, 0.5345, 0.6236]])\n"
     ]
    }
   ],
   "source": [
    "# L2 Normalization\n",
    "sample_arr = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7]]).float()\n",
    "print(f\"Shape of sample_arr: {sample_arr.shape}\")\n",
    "L2_Normalized = F.normalize(sample_arr, dim=1, p=2)\n",
    "print(f\"L2 Norm: {L2_Normalized}\")\n",
    "\n",
    "# Do it manually:\n",
    "L2_norm = torch.norm(sample_arr, dim=1, p=2)\n",
    "print(f\"L2 Norm itself: {L2_norm}\")\n",
    "# divide by it:\n",
    "print(\n",
    "    f\"Divided by L2 Norm: {sample_arr / L2_norm.unsqueeze(-1).expand(sample_arr.shape)}\"\n",
    ")\n",
    "\n",
    "# You can compute the norm itself manually:\n",
    "l2_norm_manually = torch.sqrt(torch.sum(sample_arr**2, dim=1))\n",
    "print(f\"L2 Norm manually: {l2_norm_manually}\")\n",
    "print(\n",
    "    \"Divided by L2 Norm manually: \",\n",
    "    sample_arr / l2_norm_manually.unsqueeze(-1).expand(sample_arr.shape),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a89731",
   "metadata": {},
   "source": [
    "# Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6b207ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6f01def6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b90892a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
