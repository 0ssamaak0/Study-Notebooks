{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Sentiment in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scentences = [\"I love my dog\", \"I love my cat\", \"you love my dog!\"]\n",
    "\n",
    "# <OOV>: <UNK>\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(scentences)\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scentences.append(\"love love love my dog dog dog he is amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5], [2, 2, 2, 3, 5, 5, 5, 1, 1, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(scentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 3, 5, 0, 0, 0],\n",
       "       [4, 2, 3, 6, 0, 0, 0],\n",
       "       [7, 2, 3, 5, 0, 0, 0],\n",
       "       [3, 5, 5, 5, 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding = \"post\", maxlen=7, truncating=\"pre\")\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# !wget https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"./sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "# Initialize lists\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "# Append elements in the dictionaries into each list\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26709\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(len(sequences))\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset using `tensorflow_datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf048b719994403aeb1ab49aea89c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811f1db6374f499eb00f837bf3185634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9068daaecfac42e9882ff5f51390adeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f59b17d0294266aeeeaf6e1e0995a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b568772a0914cb79bc4e51159051caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQELMI0/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f30c56487a46fb8b4d2f975cef323d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f13ee0a6d72419294a913672bee487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQELMI0/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b38c7e43214c0cad6ac266f38dc87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9047f411a1f84ade80e2208b80ae7ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQELMI0/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 19:53:16.725622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:16.810432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:16.810991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:16.812960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-20 19:53:16.817447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:16.818437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:16.819492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:18.270810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:18.271048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:18.271058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-09-20 19:53:18.271261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-20 19:53:18.271349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3943 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# imdb, info = tfds.load(\"imdb_reviews\", with_info= True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, Split('test'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, Split('unsupervised'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "print(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "[0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences[0])\n",
    "print(training_labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = imdb[\"train\"], imdb[\"test\"]\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s, l in training_data:\n",
    "    training_sentences.append(s.numpy().decode(\"utf8\"))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "for s, l in testing_data:\n",
    "    testing_sentences.append(s.numpy().decode(\"utf8\"))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "testing_labels = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "vocab_size = int(1e4)\n",
    "max_length = 120\n",
    "# Embedding vector length\n",
    "embedding_dim = 16\n",
    "trunc_type = \"post\"\n",
    "oov_token = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token = oov_token)\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = tf.keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen = max_length, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "[   0    0    0   12   14   33  425  392   18   90   28    1    9   32\n",
      " 1366 3585   40  486    1  197   24   85  154   19   12  213  329   28\n",
      "   66  247  215    9  477   58   66   85  114   98   22 5675   12 1322\n",
      "  643  767   12   18    7   33  400 8170  176 2455  416    2   89 1231\n",
      "  137   69  146   52    2    1 7577   69  229   66 2933   16    1 2904\n",
      "    1    1 1479 4940    3   39 3900  117 1584   17 3585   14  162   19\n",
      "    4 1231  917 7917    9    4   18   13   14 4139    5   99  145 1214\n",
      "   11  242  683   13   48   24  100   38   12 7181 5515   38 1366    1\n",
      "   50  401   11   98 1197  867  141   10]\n",
      "[0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences[0])\n",
    "print(padded[0])\n",
    "print(training_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_5   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.1251 - accuracy: 0.9600 - val_loss: 0.6014 - val_accuracy: 0.8075\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.1151 - accuracy: 0.9636 - val_loss: 0.6429 - val_accuracy: 0.8009\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1050 - accuracy: 0.9678 - val_loss: 0.6903 - val_accuracy: 0.7964\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0953 - accuracy: 0.9732 - val_loss: 0.7466 - val_accuracy: 0.7986\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.0854 - accuracy: 0.9767 - val_loss: 0.8094 - val_accuracy: 0.7949\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0780 - accuracy: 0.9795 - val_loss: 0.8786 - val_accuracy: 0.7928\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0705 - accuracy: 0.9827 - val_loss: 0.8951 - val_accuracy: 0.7896\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0642 - accuracy: 0.9851 - val_loss: 0.9537 - val_accuracy: 0.7860\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0582 - accuracy: 0.9868 - val_loss: 1.0382 - val_accuracy: 0.7864\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0520 - accuracy: 0.9890 - val_loss: 1.1441 - val_accuracy: 0.7859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32dc564f40>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    padded,\n",
    "    training_labels,\n",
    "    epochs = 10,\n",
    "    validation_data = (testing_padded, testing_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving weights for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 16)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights = model.layers[0].get_weights()[0]\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open(\"vecs.tsv\", \"w\", encoding= \"utf-8\")\n",
    "out_m = io.open(\"meta.tsv\", \"w\", encoding= \"utf-8\")\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word_embedding = embedding_weights[word_num]\n",
    "    word_name = tokenizer.index_word[word_num]\n",
    "\n",
    "    out_m.write(word_name + \"\\n\")\n",
    "    out_v.write(\"\\t\".join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub words tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ten sor Fl ow from basic cs to master`\n",
    "- case sensitive\n",
    "- punctuation remains\n",
    "\n",
    "➡️ better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM / GRU Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tf.keras.layers.Input(shape = (3,1))\n",
    "# return_sequences = True ➡️ 3 hidden states\n",
    "lstm2 = tf.keras.layers.LSTM(1, return_sequences = True)(inputs2)\n",
    "model = tf.keras.models.Model(inputs2, lstm2)\n",
    "\n",
    "model.predict(np.array([0.1,0.2,0.3]).reshape(1,3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    # return_seuqnces: output of the first matches the input of the second\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(6, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- Using different kinds of layers often causes overfitting (LSTM vs NonLSTM Example)\n",
    "- Conv1D can be used, and it's faster\n",
    "- overfitting often happens in text because some <UNK> words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were \\n proud to say that they were perfectly normal, thank \\n you very much. ' ey were the last people you'd expect to be in'volved in anything strange or mysterious, because they just didn't \\n hold with such nonsense.\\n Mr. Dursley was the director of a fi rm called Grunnings, which \\n made drills. He was a big, beefy man with hardly any neck, al'though he did have a very large mustache. Mrs. Dursley was thin \\n and blonde and had nearly twice the usual amount of neck, which \\n came in very useful as she spent so much of her time craning over \\n garden fences, spying on the neighbors. ' e Dursleys had a small \\n son called Dudley and in their opinion there was no fi ner boy \\n anywhere.\\n ' e Dursleys had everything they wanted, but they also had a \\n secret, and their greatest fear was that somebody would discover it. \\n ' ey didn't think they could bear it if anyone found out about the \\n Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met \\n for several years; in fact, Mrs. Dursley pretended she didn't have a \\n sister, because her sister and her good-for-nothing husband were \\n as unDursleyish as it was possible to be. ' e Dursleys shuddered \\n to think what the neighbors would say if the Potters arrived in the \\n street. ' e Dursleys knew that the Potters had a small son, too, but \\n they had never even seen him. ' is boy was another good reason \\n for keeping the Potters away; they didn't want Dudley mixing with \\n a child like that.\\n When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday \\n our story starts, there was nothing about the cloudy sky outside to \\n suggest that strange and mysterious things would soon be happen'ing all over the country. Mr. Dursley hummed as he picked out his \\n most boring tie for work, and Mrs. Dursley gossiped away happily \\n as she wrestled a screaming Dudley into his high chair.\\n None of them noticed a large, tawny owl fl utter past the \\n window.\\n At half past eight, Mr. Dursley picked up his briefcase, pecked \\n Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but \\n missed, because Dudley was now having a tantrum and throwing \\n his cereal at the walls. “Little tyke,'' chortled Mr. Dursley as he left \\n the house. He got into his car and backed out of number four's \\n drive.\\n It was on the corner of the street that he noticed the fi rst sign \\n of something peculiar — a cat reading a map. For a second, Mr. \\n Dursley didn't realize what he had seen — then he jerked his head \\n around to look again. ' ere was a tabby cat standing on the corner \\n of Privet Drive, but there wasn't a map in sight. What could he \\n have been thinking of? It must have been a trick of the light. Mr. \\n Dursley blinked and stared at the cat. It stared back. As Mr. Durs'ley drove around the corner and up the road, he watched the cat in \\n his mirror. It was now reading the sign that said Privet Drive — no, \\n looking at the sign; cats couldn't read maps or signs. Mr. Dursley \\n gave himself a little shake and put the cat out of his mind. As he \\n drove toward town he thought of nothing except a large order of \\n drills he was hoping to get that day.\\n But on the edge of town, drills were driven out of his mind \\n by something else. As he sat in the usual morning traffi c jam, he \\n couldn't help noticing that there seemed to be a lot of strangely \\n dressed people about. People in cloaks. Mr. Dursley couldn't bear \\n people who dressed in funny clothes — the getups you saw on \\n young people! He supposed this was some stupid new fashion. He \\n drummed his fi ngers on the steering wheel and his eyes fell on a \\n huddle of these weirdos standing quite close by. ' ey were whis'pering excitedly together. Mr. Dursley was enraged to see that a \\n couple of them weren't young at all; why, that man had to be older \\n than he was, and wearing an emerald-green cloak! ' e nerve of \\n him! But then it struck Mr. Dursley that this was probably some \\n silly stunt — these people were obviously collecting for something \\n . . . yes, that would be it. ' e traffi c moved on and a few minutes \\n later, Mr. Dursley arrived in the Grunnings parking lot, his mind \\n back on drills.\\n Mr. Dursley always sat with his back to the window in his offi ce \\n on the ninth fl oor. If he hadn't, he might have found it harder to \\n concentrate on drills that morning. He didn't see the owls swooping \\n past in broad daylight, though people down in the street did; they \\n pointed and gazed open-mouthed as owl after owl sped overhead. \\n Most of them had never seen an owl even at nighttime. Mr. Durs'ley, however, had a perfectly normal, owl-free morning. He yelled \\n at fi ve diff erent people. He made several important telephone calls \\n and shouted a bit more. He was in a very good mood until lunch'time, when he thought he'd stretch his legs and walk across the \\n road to buy himself a bun from the bakery.\\n He'd forgotten all about the people in cloaks until he passed \\n a group of them next to the baker's. He eyed them angrily as he \\n passed. He didn't know why, but they made him uneasy. ' is \\n bunch were whispering excitedly, too, and he couldn't see a single \\n collecting tin. It was on his way back past them, clutching a large \\n doughnut in a bag, that he caught a few words of what they were \\n saying.\\n “' e Potters, that's right, that's what I heard —''\\n “— yes, their son, Harry —''\\n Mr. Dursley stopped dead. Fear fl ooded him. He looked back \\n at the whisperers as if he wanted to say something to them, but \\n thought better of it.\\n He dashed back across the road, hurried up to his offi ce, snapped \\n at his secretary not to disturb him, seized his telephone, and had \\n almost fi nished dialing his home number when he changed his \\n mind. He put the receiver back down and stroked his mustache, \\n thinking . . . no, he was being stupid. Potter wasn't such an un'usual name. He was sure there were lots of people called Potter who \\n had a son called Harry. Come to think of it, he wasn't even sure his \\n nephew was called Harry. He'd never even seen the boy. It might \\n have been Harvey. Or Harold. ' ere was no point in worrying \\n Mrs. Dursley; she always got so upset at any mention of her sister. \\n He didn't blame her — if he'd had a sister like that . . . but all the \\n same, those people in cloaks . . . \\n He found it a lot harder to concentrate on drills that afternoon \\n and when he left the building at fi ve o'clock, he was still so worried \\n that he walked straight into someone just outside the door.\\n “Sorry,'' he grunted, as the tiny old man stumbled and almost \\n fell. It was a few seconds before Mr. Dursley realized that the man \\n was wearing a violet cloak. He didn't seem at all upset at being al'most knocked to the ground. On the contrary, his face split into a \\n wide smile and he said in a squeaky voice that made passersby stare, \\n “Don't be sorry, my dear sir, for nothing could upset me today! Re'joice, for You-Know-Who has gone at last! Even Muggles like your'self should be celebrating, this happy, happy day!''\\n And the old man hugged Mr. Dursley around the middle and \\n walked off .\\n Mr. Dursley stood rooted to the spot. He had been hugged by a \\n complete stranger. He also thought he had been called a Muggle, \\n whatever that was. He was rattled. He hurried to his car and set \\n off for home, hoping he was imagining things, which he had never \\n hoped before, because he didn't approve of imagination.\\n As he pulled into the driveway of number four, the fi rst thing he \\n saw — and it didn't improve his mood — was the tabby cat he'd \\n spotted that morning. It was now sitting on his garden wall. He \\n was sure it was the same one; it had the same markings around its \\n eyes.\\n “Shoo!'' said Mr. Dursley loudly.\\n ' e cat didn't move. It just gave him a stern look. Was this nor'mal cat behavior? Mr. Dursley wondered. Trying to pull himself to'gether, he let himself into the house. He was still determined not to \\n mention anything to his wife.\\n Mrs. Dursley had had a nice, normal day. She told him over din'ner all about Mrs. Next Door's problems with her daughter and how \\n Dudley had learned a new word (“Won't!''). Mr. Dursley tried to \\n act normally. When Dudley had been put to bed, he went into the \\n living room in time to catch the last report on the evening news:\\n “And fi nally, bird-watchers everywhere have reported that the \\n nation's owls have been behaving very unusually today. Although \\n owls normally hunt at night and are hardly ever seen in daylight, \\n there have been hundreds of sightings of these birds fl ying in every \\n direction since sunrise. Experts are unable to explain why the owls \\n have suddenly changed their sleeping pattern.'' ' e newscaster \\n allowed himself a grin. “Most mysterious. And now, over to Jim \\n McGuffi n with the weather. Going to be any more showers of owls \\n tonight, Jim?''\\n “Well, Ted,'' said the weatherman, “I don't know about that, but \\n it's not only the owls that have been acting oddly today. Viewers as \\n far apart as Kent, Yorkshire, and Dundee have been phoning in to \\n tell me that instead of the rain I promised yesterday, they've had a \\n downpour of shooting stars! Perhaps people have been celebrating \\n Bonfi re Night early — it's not until next week, folks! But I can \\n promise a wet night tonight.''\\n Mr. Dursley sat frozen in his armchair. Shooting stars all over \\n Britain? Owls fl ying by daylight? Mysterious people in cloaks all \\n over the place? And a whisper, a whisper about the Potters . . . \\n Mrs. Dursley came into the living room carrying two cups of \\n tea. It was no good. He'd have to say something to her. He cleared \\n his throat nervously. “Er — Petunia, dear — you haven't heard \\n from your sister lately, have you?''\\n As he had expected, Mrs. Dursley looked shocked and angry. Af'ter all, they normally pretended she didn't have a sister.\\n “No,'' she said sharply. “Why?''\\n “Funny stuff on the news,'' Mr. Dursley mumbled. “Owls . . . \\n shooting stars . . . and there were a lot of funny-looking people in \\n town today . . .''\\n “So?'' snapped Mrs. Dursley.\\n “Well, I just thought . . . maybe . . . it was something to do \\n with . . . you know . . . her crowd.''\\n Mrs. Dursley sipped her tea through pursed lips. Mr. Dursley \\n wondered whether he dared tell her he'd heard the name “Potter.'' \\n He decided he didn't dare. Instead he said, as casually as he could, \\n “' eir son — he'd be about Dudley's age now, wouldn't he?''\\n “I suppose so,'' said Mrs. Dursley stiffl y.\\n “What's his name again? Howard, isn't it?''\\n “Harry. Nasty, common name, if you ask me.''\\n “Oh, yes,'' said Mr. Dursley, his heart sinking horribly. “Yes, I \\n quite agree.''\\n He didn't say another word on the subject as they went upstairs \\n to bed. While Mrs. Dursley was in the bathroom, Mr. Dursley \\n crept to the bedroom window and peered down into the front gar'den. ' e cat was still there. It was staring down Privet Drive as \\n though it were waiting for something.\\n Was he imagining things? Could all this have anything to do \\n with the Potters? If it did . . . if it got out that they were related to \\n a pair of — well, he didn't think he could bear it.\\n ' e Dursleys got into bed. Mrs. Dursley fell asleep quickly but \\n Mr. Dursley lay awake, turning it all over in his mind. His last, \\n comforting thought before he fell asleep was that even if the Potters \\n were involved, there was no reason for them to come near him and \\n Mrs. Dursley. ' e Potters knew very well what he and Petunia \\n thought about them and their kind. . . . He couldn't see how he \\n and Petunia could get mixed up in anything that might be going \\n on — he yawned and turned over — it couldn't aff ect them. . . . \\n How very wrong he was.\\n Mr. Dursley might have been drifting into an uneasy sleep, but \\n the cat on the wall outside was showing no sign of sleepiness. It was \\n sitting as still as a statue, its eyes fi xed unblinkingly on the far cor'ner of Privet Drive. It didn't so much as quiver when a car door \\n slammed on the next street, nor when two owls swooped overhead. \\n In fact, it was nearly midnight before the cat moved at all.\\n A man appeared on the corner the cat had been watching, ap'peared so suddenly and silently you'd have thought he'd just \\n popped out of the ground. ' e cat's tail twitched and its eyes nar'rowed.\\n Nothing like this man had ever been seen on Privet Drive. He \\n was tall, thin, and very old, judging by the silver of his hair and \\n beard, which were both long enough to tuck into his belt. He was \\n wearing long robes, a purple cloak that swept the ground, and \\n high-heeled, buckled boots. His blue eyes were light, bright, and \\n sparkling behind half-moon spectacles and his nose was very long \\n and crooked, as though it had been broken at least twice. ' is \\n man's name was Albus Dum ble dore.\\n Albus Dum ble dore didn't seem to realize that he had just arrived \\n in a street where everything from his name to his boots was unwel'come. He was busy rummaging in his cloak, looking for some'thing. But he did seem to realize he was being watched, because he \\n looked up suddenly at the cat, which was still staring at him from \\n the other end of the street. For some reason, the sight of the cat \\n seemed to amuse him. He chuckled and muttered, “I should have \\n known.''\\n He found what he was looking for in his inside pocket. It \\n seemed to be a silver cigarette lighter. He fl icked it open, held it up \\n in the air, and clicked it. ' e nearest street lamp went out with a \\n little pop. He clicked it again — the next lamp fl ickered into dark'ness. Twelve times he clicked the Put-Outer, until the only lights \\n left on the whole street were two tiny pinpricks in the distance, \\n which were the eyes of the cat watching him. If anyone looked out \\n of their window now, even beady-eyed Mrs. Dursley, they wouldn't \\n be able to see anything that was happening down on the pavement. \\n Dum ble dore slipped the Put-Outer back inside his cloak and set \\n off down the street toward number four, where he sat down on the \\n wall next to the cat. He didn't look at it, but after a moment he \\n spoke to it.\\n “Fancy seeing you here, Professor McGonagall.''\\n He turned to smile at the tabby, but it had gone. Instead he was \\n smiling at a rather severe-looking woman who was wearing square \\n glasses exactly the shape of the markings the cat had had around its \\n eyes. She, too, was wearing a cloak, an emerald one. Her black hair \\n was drawn into a tight bun. She looked distinctly ruffl ed.\\n “How did you know it was me?'' she asked.\\n “My dear Professor, I've never seen a cat sit so stiffl y.''\\n “You'd be stiff if you'd been sitting on a brick wall all day,'' said \\n Professor McGonagall.\\n “All day? When you could have been celebrating? I must have \\n passed a dozen feasts and parties on my way here.''\\n Professor McGonagall sniff ed angrily.\\n “Oh yes, everyone's celebrating, all right,'' she said impatiently. \\n “You'd think they'd be a bit more careful, but no — even the Mug'gles have noticed something's going on. It was on their news.'' She \\n jerked her head back at the Dursleys' dark living-room window. “I \\n heard it. Flocks of owls . . . shooting stars. . . . Well, they're not \\n completely stupid. ' ey were bound to notice something. Shoot'ing stars down in Kent — I'll bet that was Dedalus Diggle. He \\n never had much sense.''\\n “You can't blame them,'' said Dum ble dore gently. “We've had \\n precious little to celebrate for eleven years.''\\n “I know that,'' said Professor McGonagall irritably. “But that's \\n no reason to lose our heads. People are being downright careless, \\n out on the streets in broad daylight, not even dressed in Muggle \\n clothes, swapping rumors.''\\n She threw a sharp, sideways glance at Dum ble dore here, as \\n though hoping he was going to tell her something, but he didn't, so \\n she went on. “A fi ne thing it would be if, on the very day You'Know-Who seems to have disappeared at last, the Muggles found \\n out about us all. I suppose he really has gone, Dum ble dore?''\\n “It certainly seems so,'' said Dum ble dore. “We have much to be \\n thankful for. Would you care for a lemon drop?''\\n “A what?''\\n “A lemon drop. ' ey're a kind of Muggle sweet I'm rather \\n fond of.''\\n “No, thank you,'' said Professor McGonagall coldly, as though \\n she didn't think this was the moment for lemon drops. “As I say, \\n even if You-Know-Who has gone —''\\n “My dear Professor, surely a sensible person like yourself can call \\n him by his name? All this 'You-Know-Who' nonsense — for eleven \\n years I have been trying to persuade people to call him by his \\n proper name: Vol de mort.'' Professor McGonagall fl inched, but Dum'bledore, who was unsticking two lemon drops, seemed not to no'tice. “It all gets so confusing if we keep saying 'You-Know-Who.' I \\n have never seen any reason to be frightened of saying Vol de mort's \\n name.''\\n “I know you haven't,'' said Professor McGonagall, sounding half \\n exasperated, half admiring. “But you're diff erent. Everyone knows \\n you're the only one You-Know- oh, all right, Vol de mort, was fright'ened of.''\\n “You fl atter me,'' said Dum ble dore calmly. “Vol de mort had \\n powers I will never have.''\\n “Only because you're too — well — noble to use them.''\\n “It's lucky it's dark. I haven't blushed so much since Madam \\n Pomfrey told me she liked my new earmuff s.''\\n Professor McGonagall shot a sharp look at Dum ble dore and \\n said, “' e owls are nothing next to the rumors that are fl ying \\n around. You know what everyone's saying? About why he's disap'peared? About what fi nally stopped him?''\\n It seemed that Professor McGonagall had reached the point she \\n was most anxious to discuss, the real reason she had been waiting \\n on a cold, hard wall all day, for neither as a cat nor as a woman had \\n she fi xed Dum ble dore with such a piercing stare as she did now. It \\n was plain that whatever “everyone'' was saying, she was not going \\n to believe it until Dum ble dore told her it was true. Dum ble dore, \\n however, was choosing another lemon drop and did not answer.\\n “What they're saying,'' she pressed on, “is that last night Vol'demort turned up in Godric's Hollow. He went to fi nd the Pot'ters. ' e rumor is that Lily and James Potter are — are — that \\n they're — dead.''\\n Dum ble dore bowed his head. Professor McGonagall gasped.\\n “Lily and James . . . I can't believe it . . . I didn't want to believe \\n it . . . Oh, Albus . . .''\\n Dum ble dore reached out and patted her on the shoulder. “I \\n know . . . I know . . .'' he said heavily.\\n Professor McGonagall's voice trembled as she went on. “' at's \\n not all. ' ey're saying he tried to kill the Potters' son, Harry. \\n But — he couldn't. He couldn't kill that little boy. No one knows \\n why, or how, but they're saying that when he couldn't kill Harry \\n Potter, Vol de mort's power somehow broke — and that's why he's \\n gone.''\\n Dum ble dore nodded glumly.\\n “It's — it's true?'' faltered Professor McGonagall. “After all he's \\n done . . . all the people he's killed . . . he couldn't kill a little boy? \\n It's just astounding . . . of all the things to stop him . . . but how in \\n the name of heaven did Harry survive?''\\n “We can only guess,'' said Dum ble dore. “We may never know.''\\n Professor McGonagall pulled out a lace handkerchief and \\n dabbed at her eyes beneath her spectacles. Dum ble dore gave a great \\n sniff as he took a golden watch from his pocket and examined it. \\n It was a very odd watch. It had twelve hands but no numbers; in'stead, little planets were moving around the edge. It must have \\n made sense to Dum ble dore, though, because he put it back in his \\n pocket and said, “Hagrid's late. I suppose it was he who told you I'd \\n be here, by the way?''\\n “Yes,'' said Professor McGonagall. “And I don't suppose you're \\n going to tell me why you're here, of all places?''\\n “I've come to bring Harry to his aunt and uncle. ' ey're the \\n only family he has left now.''\\n “You don't mean — you can't mean the people who live here?'' \\n cried Professor McGonagall, jumping to her feet and pointing at \\n number four. “Dum ble dore — you can't. I've been watching them \\n all day. You couldn't fi nd two people who are less like us. And \\n they've got this son — I saw him kicking his mother all the way up \\n the street, screaming for sweets. Harry Potter come and live here!''\\n “It's the best place for him,'' said Dum ble dore fi rmly. “His aunt \\n and uncle will be able to explain everything to him when he's older. \\n I've written them a letter.''\\n “A letter?'' repeated Professor McGonagall faintly, sitting back \\n down on the wall. “Really, Dum ble dore, you think you can explain \\n all this in a letter? ' ese people will never understand him! He'll be \\n famous — a legend — I wouldn't be surprised if today was known \\n as Harry Potter Day in the future — there will be books written \\n about Harry — every child in our world will know his name!''\\n “Exactly,'' said Dum ble dore, looking very seriously over the top \\n of his half-moon glasses. “It would be enough to turn any boy's \\n head. Famous before he can walk and talk! Famous for something \\n he won't even remember! Can't you see how much better off he'll \\n be, growing up away from all that until he's ready to take it?''\\n Professor McGonagall opened her mouth, changed her mind, \\n swallowed, and then said, “Yes — yes, you're right, of course. But \\n how is the boy getting here, Dum ble dore?'' She eyed his cloak sud'denly as though she thought he might be hiding Harry underneath \\n it.\\n “Hagrid's bringing him.''\\n “You think it — wise — to trust Hagrid with something as im'portant as this?''\\n “I would trust Hagrid with my life,'' said Dum ble dore.\\n “I'm not saying his heart isn't in the right place,'' said Professor \\n McGonagall grudgingly, “but you can't pretend he's not careless. \\n He does tend to — what was that?''\\n A low rumbling sound had broken the silence around them. It \\n grew steadily louder as they looked up and down the street for \\n some sign of a headlight; it swelled to a roar as they both looked up \\n at the sky — and a huge motorcycle fell out of the air and landed \\n on the road in front of them.\\n If the motorcycle was huge, it was nothing to the man sitting \\n astride it. He was almost twice as tall as a normal man and at least \\n fi ve times as wide. He looked simply too big to be allowed, and so \\n wild — long tangles of bushy black hair and beard hid most of his \\n face, he had hands the size of trash can lids, and his feet in their \\n leather boots were like baby dolphins. In his vast, muscular arms he \\n was holding a bundle of blankets.\\n “Hagrid,'' said Dum ble dore, sounding relieved. “At last. And \\n where did you get that motorcycle?''\\n “Borrowed it, Professor Dum ble dore, sir,'' said the giant, climb'ing carefully off the motorcycle as he spoke. “Young Sirius Black \\n lent it to me. I've got him, sir.''\\n “No problems, were there?''\\n “No, sir — house was almost destroyed, but I got him out all \\n right before the Muggles started swarmin' around. He fell asleep as \\n we was fl yin' over Bristol.''\\n Dum ble dore and Professor McGonagall bent forward over the \\n bundle of blankets. Inside, just visible, was a baby boy, fast asleep. \\n Under a tuft of jet-black hair over his forehead they could see a cu'riously shaped cut, like a bolt of lightning.\\n “Is that where — ?'' whispered Professor McGonagall.\\n “Yes,'' said Dum ble dore. “He'll have that scar forever.''\\n “Couldn't you do something about it, Dum ble dore?''\\n “Even if I could, I wouldn't. Scars can come in handy. I have one \\n myself above my left knee that is a perfect map of the London Un'derground. Well — give him here, Hagrid — we'd better get this \\n over with.''\\n Dum ble dore took Harry in his arms and turned toward the \\n Dursleys' house.\\n “Could I — could I say good-bye to him, sir?'' asked Hagrid. He \\n bent his great, shaggy head over Harry and gave him what must \\n have been a very scratchy, whiskery kiss. ' en, suddenly, Hagrid \\n let out a howl like a wounded dog.\\n “Shhh!'' hissed Professor McGonagall, “you'll wake the Mug'gles!''\\n “S-s-sorry,'' sobbed Hagrid, taking out a large, spotted handker'chief and burying his face in it. “But I c-c-can't stand it — Lily an' \\n James dead — an' poor little Harry off ter live with Muggles —''\\n “Yes, yes, it's all very sad, but get a grip on yourself, Hagrid, or \\n we'll be found,'' Professor McGonagall whispered, patting Hagrid \\n gingerly on the arm as Dum ble dore stepped over the low garden \\n wall and walked to the front door. He laid Harry gently on the \\n doorstep, took a letter out of his cloak, tucked it inside Harry's \\n blankets, and then came back to the other two. For a full minute \\n the three of them stood and looked at the little bundle; Hagrid's \\n shoulders shook, Professor McGonagall blinked furiously, and the \\n twinkling light that usually shone from Dum ble dore's eyes seemed \\n to have gone out.\\n “Well,'' said Dum ble dore fi nally, “that's that. We've no business \\n staying here. We may as well go and join the celebrations.''\\n “Yeah,'' said Hagrid in a very muffl ed voice, “I'd best get this \\n bike away. G'night, Professor McGonagall — Professor Dum'bledore, sir.''\\n Wiping his streaming eyes on his jacket sleeve, Hagrid swung \\n himself onto the motorcycle and kicked the engine into life; with a \\n roar it rose into the air and off into the night.\\n “I shall see you soon, I expect, Professor McGonagall,'' said \\n Dum ble dore, nodding to her. Professor McGonagall blew her nose \\n in reply.\\n Dum ble dore turned and walked back down the street. On the \\n corner he stopped and took out the silver Put-Outer. He clicked it \\n once, and twelve balls of light sped back to their street lamps so \\n that Privet Drive glowed suddenly orange and he could make out a \\n tabby cat slinking around the corner at the other end of the street. \\n He could just see the bundle of blankets on the step of number \\n four.\\n “Good luck, Harry,'' he murmured. He turned on his heel and \\n with a swish of his cloak, he was gone.\\n A breeze ruffl ed the neat hedges of Privet Drive, which lay silent \\n and tidy under the inky sky, the very last place you would expect \\n astonishing things to happen. Harry Potter rolled over inside his \\n blankets without waking up. One small hand closed on the letter \\n beside him and he slept on, not knowing he was special, not know'ing he was famous, not knowing he would be woken in a few \\n hours' time by Mrs. Dursley's scream as she opened the front door \\n to put out the milk bottles, nor that he would spend the next few \\n weeks being prodded and pinched by his cousin Dudley. . . . He \\n couldn't know that at this very moment, people meeting in secret \\n all over the country were holding up their glasses and saying in \\n hushed voices: “To Harry Potter — the boy who lived!''\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "print(len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr. and mrs. dursley, of number four, privet drive, were ']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "\n",
    "        n_gram_sequences = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "1276\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "total_words = len(tokenizer.index_word) + 1\n",
    "\n",
    "print(max_sequence_len)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# one-hot encoding\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mr.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/TensorFlow Specialization/3- NLP - Toeknization.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/TensorFlow%20Specialization/3-%20NLP%20-%20Toeknization.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mword_index[\u001b[39m\"\u001b[39;49m\u001b[39mmr.\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mr.'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index[\"mr.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr.', 'and', 'mrs.', 'dursley,', 'of', 'number', 'four,', 'privet', 'drive,', 'were']\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  25   5  40  16   8 110\n",
      " 155]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "print(corpus[0].split())\n",
    "# print([tokenizer.word_index[word] for word in corpus[0].split()])\n",
    "print(xs[6])\n",
    "# print(ys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "142/142 [==============================] - 5s 19ms/step - loss: 6.5104 - accuracy: 0.0334\n",
      "Epoch 2/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 6.0394 - accuracy: 0.0401\n",
      "Epoch 3/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 5.9225 - accuracy: 0.0398\n",
      "Epoch 4/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 5.8361 - accuracy: 0.0390\n",
      "Epoch 5/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 5.7660 - accuracy: 0.0401\n",
      "Epoch 6/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 5.7058 - accuracy: 0.0467\n",
      "Epoch 7/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 5.6509 - accuracy: 0.0509\n",
      "Epoch 8/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 5.5988 - accuracy: 0.0560\n",
      "Epoch 9/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 5.5492 - accuracy: 0.0587\n",
      "Epoch 10/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 5.5012 - accuracy: 0.0642\n",
      "Epoch 11/500\n",
      "142/142 [==============================] - 2s 18ms/step - loss: 5.4547 - accuracy: 0.0666\n",
      "Epoch 12/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 5.4072 - accuracy: 0.0664\n",
      "Epoch 13/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 5.3571 - accuracy: 0.0702\n",
      "Epoch 14/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 5.3018 - accuracy: 0.0766\n",
      "Epoch 15/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 5.2454 - accuracy: 0.0812\n",
      "Epoch 16/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 5.1874 - accuracy: 0.0868\n",
      "Epoch 17/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 5.1283 - accuracy: 0.1012\n",
      "Epoch 18/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 5.0687 - accuracy: 0.1091\n",
      "Epoch 19/500\n",
      "142/142 [==============================] - 2s 18ms/step - loss: 5.0056 - accuracy: 0.1184\n",
      "Epoch 20/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 4.9428 - accuracy: 0.1335\n",
      "Epoch 21/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 4.8790 - accuracy: 0.1430\n",
      "Epoch 22/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 4.8126 - accuracy: 0.1446\n",
      "Epoch 23/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 4.7463 - accuracy: 0.1614\n",
      "Epoch 24/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 4.6803 - accuracy: 0.1671\n",
      "Epoch 25/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 4.6123 - accuracy: 0.1725\n",
      "Epoch 26/500\n",
      "142/142 [==============================] - 2s 18ms/step - loss: 4.5432 - accuracy: 0.1809\n",
      "Epoch 27/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 4.4748 - accuracy: 0.1880\n",
      "Epoch 28/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 4.4045 - accuracy: 0.2001\n",
      "Epoch 29/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 4.3352 - accuracy: 0.2041\n",
      "Epoch 30/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 4.2679 - accuracy: 0.2112\n",
      "Epoch 31/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 4.1991 - accuracy: 0.2205\n",
      "Epoch 32/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 4.1320 - accuracy: 0.2196\n",
      "Epoch 33/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 4.0665 - accuracy: 0.2285\n",
      "Epoch 34/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.9998 - accuracy: 0.2446\n",
      "Epoch 35/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 3.9342 - accuracy: 0.2522\n",
      "Epoch 36/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 3.8677 - accuracy: 0.2597\n",
      "Epoch 37/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 3.8056 - accuracy: 0.2657\n",
      "Epoch 38/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 3.7422 - accuracy: 0.2794\n",
      "Epoch 39/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 3.6787 - accuracy: 0.2856\n",
      "Epoch 40/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 3.6165 - accuracy: 0.2889\n",
      "Epoch 41/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.5572 - accuracy: 0.3020\n",
      "Epoch 42/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 3.4945 - accuracy: 0.3119\n",
      "Epoch 43/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 3.4369 - accuracy: 0.3155\n",
      "Epoch 44/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 3.3775 - accuracy: 0.3283\n",
      "Epoch 45/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.3181 - accuracy: 0.3367\n",
      "Epoch 46/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 3.2625 - accuracy: 0.3462\n",
      "Epoch 47/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 3.2050 - accuracy: 0.3529\n",
      "Epoch 48/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.1499 - accuracy: 0.3637\n",
      "Epoch 49/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.0940 - accuracy: 0.3761\n",
      "Epoch 50/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 3.0400 - accuracy: 0.3828\n",
      "Epoch 51/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 2.9853 - accuracy: 0.3912\n",
      "Epoch 52/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 2.9312 - accuracy: 0.4027\n",
      "Epoch 53/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 2.8783 - accuracy: 0.4076\n",
      "Epoch 54/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 2.8311 - accuracy: 0.4173\n",
      "Epoch 55/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 2.7814 - accuracy: 0.4275\n",
      "Epoch 56/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 2.7298 - accuracy: 0.4310\n",
      "Epoch 57/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 2.6843 - accuracy: 0.4386\n",
      "Epoch 58/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.6381 - accuracy: 0.4514\n",
      "Epoch 59/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.5916 - accuracy: 0.4554\n",
      "Epoch 60/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.5483 - accuracy: 0.4636\n",
      "Epoch 61/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.5058 - accuracy: 0.4716\n",
      "Epoch 62/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 2.4623 - accuracy: 0.4817\n",
      "Epoch 63/500\n",
      "142/142 [==============================] - 2s 16ms/step - loss: 2.4204 - accuracy: 0.4875\n",
      "Epoch 64/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 2.3823 - accuracy: 0.4910\n",
      "Epoch 65/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.3408 - accuracy: 0.5037\n",
      "Epoch 66/500\n",
      "142/142 [==============================] - 2s 18ms/step - loss: 2.3025 - accuracy: 0.5085\n",
      "Epoch 67/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.2626 - accuracy: 0.5222\n",
      "Epoch 68/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 2.2253 - accuracy: 0.5242\n",
      "Epoch 69/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 2.1858 - accuracy: 0.5333\n",
      "Epoch 70/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 2.1508 - accuracy: 0.5408\n",
      "Epoch 71/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 2.1143 - accuracy: 0.5453\n",
      "Epoch 72/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 2.0757 - accuracy: 0.5541\n",
      "Epoch 73/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 2.0394 - accuracy: 0.5599\n",
      "Epoch 74/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 2.0033 - accuracy: 0.5654\n",
      "Epoch 75/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.9661 - accuracy: 0.5721\n",
      "Epoch 76/500\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 1.9333 - accuracy: 0.5809\n",
      "Epoch 77/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.9001 - accuracy: 0.5880\n",
      "Epoch 78/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.8677 - accuracy: 0.5949\n",
      "Epoch 79/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.8381 - accuracy: 0.6000\n",
      "Epoch 80/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.8065 - accuracy: 0.6053\n",
      "Epoch 81/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.7757 - accuracy: 0.6126\n",
      "Epoch 82/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.7501 - accuracy: 0.6174\n",
      "Epoch 83/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.7194 - accuracy: 0.6294\n",
      "Epoch 84/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.6937 - accuracy: 0.6318\n",
      "Epoch 85/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 1.6692 - accuracy: 0.6336\n",
      "Epoch 86/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.6437 - accuracy: 0.6391\n",
      "Epoch 87/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.6176 - accuracy: 0.6411\n",
      "Epoch 88/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.5929 - accuracy: 0.6491\n",
      "Epoch 89/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 1.5667 - accuracy: 0.6619\n",
      "Epoch 90/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.5448 - accuracy: 0.6600\n",
      "Epoch 91/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.5204 - accuracy: 0.6677\n",
      "Epoch 92/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.4979 - accuracy: 0.6699\n",
      "Epoch 93/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 1.4725 - accuracy: 0.6772\n",
      "Epoch 94/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 1.4572 - accuracy: 0.6816\n",
      "Epoch 95/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 1.4314 - accuracy: 0.6832\n",
      "Epoch 96/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 1.4117 - accuracy: 0.6947\n",
      "Epoch 97/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.3972 - accuracy: 0.6912\n",
      "Epoch 98/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.3747 - accuracy: 0.6952\n",
      "Epoch 99/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.3495 - accuracy: 0.7069\n",
      "Epoch 100/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 1.3281 - accuracy: 0.7080\n",
      "Epoch 101/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.3148 - accuracy: 0.7040\n",
      "Epoch 102/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 1.2973 - accuracy: 0.7126\n",
      "Epoch 103/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.2741 - accuracy: 0.7180\n",
      "Epoch 104/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.2626 - accuracy: 0.7184\n",
      "Epoch 105/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 1.2467 - accuracy: 0.7250\n",
      "Epoch 106/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.2220 - accuracy: 0.7292\n",
      "Epoch 107/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.2049 - accuracy: 0.7357\n",
      "Epoch 108/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.1934 - accuracy: 0.7419\n",
      "Epoch 109/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 1.1769 - accuracy: 0.7419\n",
      "Epoch 110/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 1.1623 - accuracy: 0.7470\n",
      "Epoch 111/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 1.1519 - accuracy: 0.7483\n",
      "Epoch 112/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.1343 - accuracy: 0.7512\n",
      "Epoch 113/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.1152 - accuracy: 0.7569\n",
      "Epoch 114/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.1026 - accuracy: 0.7598\n",
      "Epoch 115/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.0902 - accuracy: 0.7616\n",
      "Epoch 116/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.0771 - accuracy: 0.7627\n",
      "Epoch 117/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.0616 - accuracy: 0.7682\n",
      "Epoch 118/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 1.0446 - accuracy: 0.7737\n",
      "Epoch 119/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 1.0308 - accuracy: 0.7742\n",
      "Epoch 120/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 1.0184 - accuracy: 0.7795\n",
      "Epoch 121/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 1.0150 - accuracy: 0.7806\n",
      "Epoch 122/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.9988 - accuracy: 0.7819\n",
      "Epoch 123/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.9849 - accuracy: 0.7855\n",
      "Epoch 124/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.9684 - accuracy: 0.7906\n",
      "Epoch 125/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.9613 - accuracy: 0.7941\n",
      "Epoch 126/500\n",
      "142/142 [==============================] - 2s 18ms/step - loss: 0.9483 - accuracy: 0.7970\n",
      "Epoch 127/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.9360 - accuracy: 0.8005\n",
      "Epoch 128/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.9347 - accuracy: 0.7999\n",
      "Epoch 129/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.9270 - accuracy: 0.8041\n",
      "Epoch 130/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.9107 - accuracy: 0.8072\n",
      "Epoch 131/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8985 - accuracy: 0.8089\n",
      "Epoch 132/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8862 - accuracy: 0.8127\n",
      "Epoch 133/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8771 - accuracy: 0.8178\n",
      "Epoch 134/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.8627 - accuracy: 0.8140\n",
      "Epoch 135/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.8565 - accuracy: 0.8202\n",
      "Epoch 136/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.8456 - accuracy: 0.8198\n",
      "Epoch 137/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.8337 - accuracy: 0.8242\n",
      "Epoch 138/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8231 - accuracy: 0.8284\n",
      "Epoch 139/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8140 - accuracy: 0.8300\n",
      "Epoch 140/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.8092 - accuracy: 0.8289\n",
      "Epoch 141/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.7935 - accuracy: 0.8322\n",
      "Epoch 142/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.7890 - accuracy: 0.8326\n",
      "Epoch 143/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7766 - accuracy: 0.8377\n",
      "Epoch 144/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.7665 - accuracy: 0.8393\n",
      "Epoch 145/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7716 - accuracy: 0.8382\n",
      "Epoch 146/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.7529 - accuracy: 0.8417\n",
      "Epoch 147/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.7449 - accuracy: 0.8470\n",
      "Epoch 148/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.7441 - accuracy: 0.8470\n",
      "Epoch 149/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.7318 - accuracy: 0.8501\n",
      "Epoch 150/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.7191 - accuracy: 0.8506\n",
      "Epoch 151/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.7118 - accuracy: 0.8523\n",
      "Epoch 152/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.7014 - accuracy: 0.8574\n",
      "Epoch 153/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.6955 - accuracy: 0.8585\n",
      "Epoch 154/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.6839 - accuracy: 0.8603\n",
      "Epoch 155/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6808 - accuracy: 0.8612\n",
      "Epoch 156/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6709 - accuracy: 0.8627\n",
      "Epoch 157/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.6587 - accuracy: 0.8654\n",
      "Epoch 158/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6531 - accuracy: 0.8676\n",
      "Epoch 159/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6501 - accuracy: 0.8636\n",
      "Epoch 160/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.6455 - accuracy: 0.8674\n",
      "Epoch 161/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.6322 - accuracy: 0.8703\n",
      "Epoch 162/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6260 - accuracy: 0.8718\n",
      "Epoch 163/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.6211 - accuracy: 0.8714\n",
      "Epoch 164/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.6169 - accuracy: 0.8756\n",
      "Epoch 165/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.6088 - accuracy: 0.8734\n",
      "Epoch 166/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.6139 - accuracy: 0.8727\n",
      "Epoch 167/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.5939 - accuracy: 0.8793\n",
      "Epoch 168/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5844 - accuracy: 0.8805\n",
      "Epoch 169/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.5874 - accuracy: 0.8778\n",
      "Epoch 170/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5834 - accuracy: 0.8780\n",
      "Epoch 171/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5707 - accuracy: 0.8827\n",
      "Epoch 172/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5586 - accuracy: 0.8840\n",
      "Epoch 173/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5497 - accuracy: 0.8875\n",
      "Epoch 174/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5451 - accuracy: 0.8853\n",
      "Epoch 175/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5361 - accuracy: 0.8886\n",
      "Epoch 176/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5342 - accuracy: 0.8895\n",
      "Epoch 177/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.5263 - accuracy: 0.8937\n",
      "Epoch 178/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.5195 - accuracy: 0.8922\n",
      "Epoch 179/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5150 - accuracy: 0.8937\n",
      "Epoch 180/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5126 - accuracy: 0.8940\n",
      "Epoch 181/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.5173 - accuracy: 0.8915\n",
      "Epoch 182/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.5060 - accuracy: 0.8973\n",
      "Epoch 183/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.4988 - accuracy: 0.8966\n",
      "Epoch 184/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.4915 - accuracy: 0.8968\n",
      "Epoch 185/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4822 - accuracy: 0.9064\n",
      "Epoch 186/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.4779 - accuracy: 0.8995\n",
      "Epoch 187/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4774 - accuracy: 0.9006\n",
      "Epoch 188/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.4684 - accuracy: 0.9061\n",
      "Epoch 189/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.4650 - accuracy: 0.9050\n",
      "Epoch 190/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4681 - accuracy: 0.9013\n",
      "Epoch 191/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.4589 - accuracy: 0.9057\n",
      "Epoch 192/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.4512 - accuracy: 0.9095\n",
      "Epoch 193/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.4493 - accuracy: 0.9066\n",
      "Epoch 194/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4444 - accuracy: 0.9092\n",
      "Epoch 195/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.4519 - accuracy: 0.9068\n",
      "Epoch 196/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4393 - accuracy: 0.9101\n",
      "Epoch 197/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4392 - accuracy: 0.9148\n",
      "Epoch 198/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4275 - accuracy: 0.9121\n",
      "Epoch 199/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4179 - accuracy: 0.9141\n",
      "Epoch 200/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4090 - accuracy: 0.9172\n",
      "Epoch 201/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4083 - accuracy: 0.9185\n",
      "Epoch 202/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.4169 - accuracy: 0.9134\n",
      "Epoch 203/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.4038 - accuracy: 0.9168\n",
      "Epoch 204/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.4024 - accuracy: 0.9157\n",
      "Epoch 205/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.4017 - accuracy: 0.9188\n",
      "Epoch 206/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.3864 - accuracy: 0.9219\n",
      "Epoch 207/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.3800 - accuracy: 0.9205\n",
      "Epoch 208/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.3745 - accuracy: 0.9230\n",
      "Epoch 209/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3800 - accuracy: 0.9188\n",
      "Epoch 210/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.3759 - accuracy: 0.9236\n",
      "Epoch 211/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.3679 - accuracy: 0.9256\n",
      "Epoch 212/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.3662 - accuracy: 0.9225\n",
      "Epoch 213/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3704 - accuracy: 0.9219\n",
      "Epoch 214/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3641 - accuracy: 0.9227\n",
      "Epoch 215/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.3567 - accuracy: 0.9256\n",
      "Epoch 216/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.3501 - accuracy: 0.9269\n",
      "Epoch 217/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.3453 - accuracy: 0.9303\n",
      "Epoch 218/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.3414 - accuracy: 0.9285\n",
      "Epoch 219/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.3341 - accuracy: 0.9283\n",
      "Epoch 220/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.3313 - accuracy: 0.9323\n",
      "Epoch 221/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3278 - accuracy: 0.9329\n",
      "Epoch 222/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.3283 - accuracy: 0.9300\n",
      "Epoch 223/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.3234 - accuracy: 0.9325\n",
      "Epoch 224/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3221 - accuracy: 0.9307\n",
      "Epoch 225/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3221 - accuracy: 0.9340\n",
      "Epoch 226/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.3192 - accuracy: 0.9296\n",
      "Epoch 227/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.3170 - accuracy: 0.9307\n",
      "Epoch 228/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.3284 - accuracy: 0.9278\n",
      "Epoch 229/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.3133 - accuracy: 0.9311\n",
      "Epoch 230/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.3043 - accuracy: 0.9351\n",
      "Epoch 231/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.2974 - accuracy: 0.9362\n",
      "Epoch 232/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.2969 - accuracy: 0.9382\n",
      "Epoch 233/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.2924 - accuracy: 0.9407\n",
      "Epoch 234/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2892 - accuracy: 0.9387\n",
      "Epoch 235/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2852 - accuracy: 0.9382\n",
      "Epoch 236/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2830 - accuracy: 0.9380\n",
      "Epoch 237/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2792 - accuracy: 0.9413\n",
      "Epoch 238/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2770 - accuracy: 0.9407\n",
      "Epoch 239/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2804 - accuracy: 0.9413\n",
      "Epoch 240/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.2767 - accuracy: 0.9389\n",
      "Epoch 241/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.2861 - accuracy: 0.9362\n",
      "Epoch 242/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2799 - accuracy: 0.9407\n",
      "Epoch 243/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2735 - accuracy: 0.9411\n",
      "Epoch 244/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2693 - accuracy: 0.9440\n",
      "Epoch 245/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2740 - accuracy: 0.9389\n",
      "Epoch 246/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.2627 - accuracy: 0.9413\n",
      "Epoch 247/500\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 0.2613 - accuracy: 0.9433\n",
      "Epoch 248/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.2550 - accuracy: 0.9444\n",
      "Epoch 249/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.2502 - accuracy: 0.9449\n",
      "Epoch 250/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2476 - accuracy: 0.9449\n",
      "Epoch 251/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.2454 - accuracy: 0.9469\n",
      "Epoch 252/500\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 0.2440 - accuracy: 0.9462\n",
      "Epoch 253/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2425 - accuracy: 0.9469\n",
      "Epoch 254/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2613 - accuracy: 0.9420\n",
      "Epoch 255/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.2707 - accuracy: 0.9376\n",
      "Epoch 256/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2590 - accuracy: 0.9440\n",
      "Epoch 257/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2448 - accuracy: 0.9469\n",
      "Epoch 258/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2408 - accuracy: 0.9460\n",
      "Epoch 259/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.2329 - accuracy: 0.9491\n",
      "Epoch 260/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2301 - accuracy: 0.9489\n",
      "Epoch 261/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2279 - accuracy: 0.9482\n",
      "Epoch 262/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2257 - accuracy: 0.9495\n",
      "Epoch 263/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2241 - accuracy: 0.9497\n",
      "Epoch 264/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2204 - accuracy: 0.9500\n",
      "Epoch 265/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2173 - accuracy: 0.9497\n",
      "Epoch 266/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2178 - accuracy: 0.9473\n",
      "Epoch 267/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2188 - accuracy: 0.9491\n",
      "Epoch 268/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2155 - accuracy: 0.9502\n",
      "Epoch 269/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2112 - accuracy: 0.9486\n",
      "Epoch 270/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.2093 - accuracy: 0.9504\n",
      "Epoch 271/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.2074 - accuracy: 0.9506\n",
      "Epoch 272/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2101 - accuracy: 0.9502\n",
      "Epoch 273/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.2057 - accuracy: 0.9511\n",
      "Epoch 274/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2230 - accuracy: 0.9482\n",
      "Epoch 275/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2144 - accuracy: 0.9489\n",
      "Epoch 276/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.2095 - accuracy: 0.9493\n",
      "Epoch 277/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.2027 - accuracy: 0.9524\n",
      "Epoch 278/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.2007 - accuracy: 0.9497\n",
      "Epoch 279/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1956 - accuracy: 0.9526\n",
      "Epoch 280/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1925 - accuracy: 0.9535\n",
      "Epoch 281/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1912 - accuracy: 0.9531\n",
      "Epoch 282/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1885 - accuracy: 0.9524\n",
      "Epoch 283/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1882 - accuracy: 0.9531\n",
      "Epoch 284/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1877 - accuracy: 0.9535\n",
      "Epoch 285/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.1875 - accuracy: 0.9526\n",
      "Epoch 286/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1853 - accuracy: 0.9540\n",
      "Epoch 287/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1848 - accuracy: 0.9526\n",
      "Epoch 288/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1891 - accuracy: 0.9537\n",
      "Epoch 289/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1921 - accuracy: 0.9515\n",
      "Epoch 290/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1881 - accuracy: 0.9509\n",
      "Epoch 291/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1987 - accuracy: 0.9500\n",
      "Epoch 292/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1808 - accuracy: 0.9544\n",
      "Epoch 293/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.1749 - accuracy: 0.9542\n",
      "Epoch 294/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1714 - accuracy: 0.9544\n",
      "Epoch 295/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1697 - accuracy: 0.9555\n",
      "Epoch 296/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1675 - accuracy: 0.9562\n",
      "Epoch 297/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1676 - accuracy: 0.9557\n",
      "Epoch 298/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1662 - accuracy: 0.9548\n",
      "Epoch 299/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1718 - accuracy: 0.9542\n",
      "Epoch 300/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1732 - accuracy: 0.9544\n",
      "Epoch 301/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1794 - accuracy: 0.9517\n",
      "Epoch 302/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1861 - accuracy: 0.9504\n",
      "Epoch 303/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1695 - accuracy: 0.9551\n",
      "Epoch 304/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1645 - accuracy: 0.9557\n",
      "Epoch 305/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.1599 - accuracy: 0.9566\n",
      "Epoch 306/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1605 - accuracy: 0.9562\n",
      "Epoch 307/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1606 - accuracy: 0.9564\n",
      "Epoch 308/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1580 - accuracy: 0.9562\n",
      "Epoch 309/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1573 - accuracy: 0.9575\n",
      "Epoch 310/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1752 - accuracy: 0.9542\n",
      "Epoch 311/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1633 - accuracy: 0.9533\n",
      "Epoch 312/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1724 - accuracy: 0.9526\n",
      "Epoch 313/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1761 - accuracy: 0.9502\n",
      "Epoch 314/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1566 - accuracy: 0.9564\n",
      "Epoch 315/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1514 - accuracy: 0.9564\n",
      "Epoch 316/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1478 - accuracy: 0.9575\n",
      "Epoch 317/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1458 - accuracy: 0.9579\n",
      "Epoch 318/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1465 - accuracy: 0.9575\n",
      "Epoch 319/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1469 - accuracy: 0.9571\n",
      "Epoch 320/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1453 - accuracy: 0.9575\n",
      "Epoch 321/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1435 - accuracy: 0.9575\n",
      "Epoch 322/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1417 - accuracy: 0.9566\n",
      "Epoch 323/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1414 - accuracy: 0.9582\n",
      "Epoch 324/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1454 - accuracy: 0.9568\n",
      "Epoch 325/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1451 - accuracy: 0.9564\n",
      "Epoch 326/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1406 - accuracy: 0.9577\n",
      "Epoch 327/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1389 - accuracy: 0.9590\n",
      "Epoch 328/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1388 - accuracy: 0.9568\n",
      "Epoch 329/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1368 - accuracy: 0.9599\n",
      "Epoch 330/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1391 - accuracy: 0.9559\n",
      "Epoch 331/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1405 - accuracy: 0.9566\n",
      "Epoch 332/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1394 - accuracy: 0.9557\n",
      "Epoch 333/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1369 - accuracy: 0.9562\n",
      "Epoch 334/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1340 - accuracy: 0.9584\n",
      "Epoch 335/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1367 - accuracy: 0.9568\n",
      "Epoch 336/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1356 - accuracy: 0.9582\n",
      "Epoch 337/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1501 - accuracy: 0.9564\n",
      "Epoch 338/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1613 - accuracy: 0.9502\n",
      "Epoch 339/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1431 - accuracy: 0.9562\n",
      "Epoch 340/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1352 - accuracy: 0.9575\n",
      "Epoch 341/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1294 - accuracy: 0.9595\n",
      "Epoch 342/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1274 - accuracy: 0.9579\n",
      "Epoch 343/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1265 - accuracy: 0.9595\n",
      "Epoch 344/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1278 - accuracy: 0.9586\n",
      "Epoch 345/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1268 - accuracy: 0.9590\n",
      "Epoch 346/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1249 - accuracy: 0.9595\n",
      "Epoch 347/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1246 - accuracy: 0.9582\n",
      "Epoch 348/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1235 - accuracy: 0.9579\n",
      "Epoch 349/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1232 - accuracy: 0.9599\n",
      "Epoch 350/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1234 - accuracy: 0.9599\n",
      "Epoch 351/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1217 - accuracy: 0.9588\n",
      "Epoch 352/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1211 - accuracy: 0.9579\n",
      "Epoch 353/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1205 - accuracy: 0.9597\n",
      "Epoch 354/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1247 - accuracy: 0.9584\n",
      "Epoch 355/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1342 - accuracy: 0.9568\n",
      "Epoch 356/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1281 - accuracy: 0.9588\n",
      "Epoch 357/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1225 - accuracy: 0.9610\n",
      "Epoch 358/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1181 - accuracy: 0.9615\n",
      "Epoch 359/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1164 - accuracy: 0.9604\n",
      "Epoch 360/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1159 - accuracy: 0.9610\n",
      "Epoch 361/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1162 - accuracy: 0.9593\n",
      "Epoch 362/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1195 - accuracy: 0.9593\n",
      "Epoch 363/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1164 - accuracy: 0.9608\n",
      "Epoch 364/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1167 - accuracy: 0.9593\n",
      "Epoch 365/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1147 - accuracy: 0.9597\n",
      "Epoch 366/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1209 - accuracy: 0.9564\n",
      "Epoch 367/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1164 - accuracy: 0.9606\n",
      "Epoch 368/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1199 - accuracy: 0.9590\n",
      "Epoch 369/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1366 - accuracy: 0.9540\n",
      "Epoch 370/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1258 - accuracy: 0.9582\n",
      "Epoch 371/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1188 - accuracy: 0.9608\n",
      "Epoch 372/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1139 - accuracy: 0.9604\n",
      "Epoch 373/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1137 - accuracy: 0.9595\n",
      "Epoch 374/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1161 - accuracy: 0.9579\n",
      "Epoch 375/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1098 - accuracy: 0.9602\n",
      "Epoch 376/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1084 - accuracy: 0.9621\n",
      "Epoch 377/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1077 - accuracy: 0.9597\n",
      "Epoch 378/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1097 - accuracy: 0.9606\n",
      "Epoch 379/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1162 - accuracy: 0.9577\n",
      "Epoch 380/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1118 - accuracy: 0.9608\n",
      "Epoch 381/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1091 - accuracy: 0.9595\n",
      "Epoch 382/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1086 - accuracy: 0.9602\n",
      "Epoch 383/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1072 - accuracy: 0.9606\n",
      "Epoch 384/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1064 - accuracy: 0.9593\n",
      "Epoch 385/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1052 - accuracy: 0.9604\n",
      "Epoch 386/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1064 - accuracy: 0.9597\n",
      "Epoch 387/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1100 - accuracy: 0.9610\n",
      "Epoch 388/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1068 - accuracy: 0.9588\n",
      "Epoch 389/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1034 - accuracy: 0.9599\n",
      "Epoch 390/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1037 - accuracy: 0.9604\n",
      "Epoch 391/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.1088 - accuracy: 0.9588\n",
      "Epoch 392/500\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 0.1189 - accuracy: 0.9584\n",
      "Epoch 393/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.1254 - accuracy: 0.9546\n",
      "Epoch 394/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1276 - accuracy: 0.9564\n",
      "Epoch 395/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1091 - accuracy: 0.9593\n",
      "Epoch 396/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1022 - accuracy: 0.9613\n",
      "Epoch 397/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1011 - accuracy: 0.9606\n",
      "Epoch 398/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0998 - accuracy: 0.9606\n",
      "Epoch 399/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0997 - accuracy: 0.9608\n",
      "Epoch 400/500\n",
      "142/142 [==============================] - 3s 23ms/step - loss: 0.0991 - accuracy: 0.9613\n",
      "Epoch 401/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0992 - accuracy: 0.9606\n",
      "Epoch 402/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0988 - accuracy: 0.9590\n",
      "Epoch 403/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0982 - accuracy: 0.9606\n",
      "Epoch 404/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0984 - accuracy: 0.9615\n",
      "Epoch 405/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0985 - accuracy: 0.9610\n",
      "Epoch 406/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0986 - accuracy: 0.9599\n",
      "Epoch 407/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0984 - accuracy: 0.9588\n",
      "Epoch 408/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0983 - accuracy: 0.9602\n",
      "Epoch 409/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0967 - accuracy: 0.9602\n",
      "Epoch 410/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0968 - accuracy: 0.9608\n",
      "Epoch 411/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0985 - accuracy: 0.9606\n",
      "Epoch 412/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0973 - accuracy: 0.9610\n",
      "Epoch 413/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1009 - accuracy: 0.9615\n",
      "Epoch 414/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0973 - accuracy: 0.9595\n",
      "Epoch 415/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0978 - accuracy: 0.9599\n",
      "Epoch 416/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0952 - accuracy: 0.9595\n",
      "Epoch 417/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0977 - accuracy: 0.9606\n",
      "Epoch 418/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1312 - accuracy: 0.9515\n",
      "Epoch 419/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1479 - accuracy: 0.9491\n",
      "Epoch 420/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.1193 - accuracy: 0.9582\n",
      "Epoch 421/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.1058 - accuracy: 0.9593\n",
      "Epoch 422/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0961 - accuracy: 0.9602\n",
      "Epoch 423/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0944 - accuracy: 0.9615\n",
      "Epoch 424/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0936 - accuracy: 0.9599\n",
      "Epoch 425/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0957 - accuracy: 0.9597\n",
      "Epoch 426/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0937 - accuracy: 0.9617\n",
      "Epoch 427/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0954 - accuracy: 0.9602\n",
      "Epoch 428/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0925 - accuracy: 0.9597\n",
      "Epoch 429/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0934 - accuracy: 0.9599\n",
      "Epoch 430/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0942 - accuracy: 0.9613\n",
      "Epoch 431/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0932 - accuracy: 0.9599\n",
      "Epoch 432/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0931 - accuracy: 0.9621\n",
      "Epoch 433/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0911 - accuracy: 0.9613\n",
      "Epoch 434/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0906 - accuracy: 0.9604\n",
      "Epoch 435/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0904 - accuracy: 0.9608\n",
      "Epoch 436/500\n",
      "142/142 [==============================] - 3s 24ms/step - loss: 0.0917 - accuracy: 0.9610\n",
      "Epoch 437/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0902 - accuracy: 0.9615\n",
      "Epoch 438/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0913 - accuracy: 0.9617\n",
      "Epoch 439/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0911 - accuracy: 0.9604\n",
      "Epoch 440/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0978 - accuracy: 0.9606\n",
      "Epoch 441/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1195 - accuracy: 0.9546\n",
      "Epoch 442/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1139 - accuracy: 0.9579\n",
      "Epoch 443/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0983 - accuracy: 0.9602\n",
      "Epoch 444/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0912 - accuracy: 0.9608\n",
      "Epoch 445/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0898 - accuracy: 0.9615\n",
      "Epoch 446/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0909 - accuracy: 0.9604\n",
      "Epoch 447/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0900 - accuracy: 0.9599\n",
      "Epoch 448/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0902 - accuracy: 0.9610\n",
      "Epoch 449/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1021 - accuracy: 0.9593\n",
      "Epoch 450/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1204 - accuracy: 0.9526\n",
      "Epoch 451/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.1114 - accuracy: 0.9559\n",
      "Epoch 452/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0999 - accuracy: 0.9608\n",
      "Epoch 453/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0933 - accuracy: 0.9604\n",
      "Epoch 454/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0902 - accuracy: 0.9604\n",
      "Epoch 455/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0893 - accuracy: 0.9588\n",
      "Epoch 456/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0889 - accuracy: 0.9613\n",
      "Epoch 457/500\n",
      "142/142 [==============================] - 2s 17ms/step - loss: 0.0881 - accuracy: 0.9615\n",
      "Epoch 458/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0880 - accuracy: 0.9619\n",
      "Epoch 459/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0873 - accuracy: 0.9604\n",
      "Epoch 460/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0879 - accuracy: 0.9615\n",
      "Epoch 461/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0869 - accuracy: 0.9610\n",
      "Epoch 462/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0870 - accuracy: 0.9606\n",
      "Epoch 463/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0875 - accuracy: 0.9606\n",
      "Epoch 464/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0874 - accuracy: 0.9606\n",
      "Epoch 465/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0871 - accuracy: 0.9619\n",
      "Epoch 466/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0864 - accuracy: 0.9619\n",
      "Epoch 467/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0862 - accuracy: 0.9628\n",
      "Epoch 468/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0877 - accuracy: 0.9608\n",
      "Epoch 469/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0875 - accuracy: 0.9613\n",
      "Epoch 470/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0876 - accuracy: 0.9613\n",
      "Epoch 471/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0878 - accuracy: 0.9613\n",
      "Epoch 472/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0872 - accuracy: 0.9613\n",
      "Epoch 473/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0873 - accuracy: 0.9604\n",
      "Epoch 474/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0896 - accuracy: 0.9619\n",
      "Epoch 475/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0942 - accuracy: 0.9593\n",
      "Epoch 476/500\n",
      "142/142 [==============================] - 3s 22ms/step - loss: 0.1007 - accuracy: 0.9573\n",
      "Epoch 477/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1021 - accuracy: 0.9586\n",
      "Epoch 478/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.1025 - accuracy: 0.9577\n",
      "Epoch 479/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.1063 - accuracy: 0.9571\n",
      "Epoch 480/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0984 - accuracy: 0.9573\n",
      "Epoch 481/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0933 - accuracy: 0.9608\n",
      "Epoch 482/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0896 - accuracy: 0.9599\n",
      "Epoch 483/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0854 - accuracy: 0.9617\n",
      "Epoch 484/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0848 - accuracy: 0.9621\n",
      "Epoch 485/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0845 - accuracy: 0.9604\n",
      "Epoch 486/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0840 - accuracy: 0.9617\n",
      "Epoch 487/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0840 - accuracy: 0.9604\n",
      "Epoch 488/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0831 - accuracy: 0.9602\n",
      "Epoch 489/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0838 - accuracy: 0.9608\n",
      "Epoch 490/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0840 - accuracy: 0.9604\n",
      "Epoch 491/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0830 - accuracy: 0.9615\n",
      "Epoch 492/500\n",
      "142/142 [==============================] - 3s 18ms/step - loss: 0.0824 - accuracy: 0.9610\n",
      "Epoch 493/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0829 - accuracy: 0.9628\n",
      "Epoch 494/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0846 - accuracy: 0.9597\n",
      "Epoch 495/500\n",
      "142/142 [==============================] - 3s 21ms/step - loss: 0.0856 - accuracy: 0.9626\n",
      "Epoch 496/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0959 - accuracy: 0.9568\n",
      "Epoch 497/500\n",
      "142/142 [==============================] - 3s 20ms/step - loss: 0.0964 - accuracy: 0.9590\n",
      "Epoch 498/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0905 - accuracy: 0.9602\n",
      "Epoch 499/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0864 - accuracy: 0.9608\n",
      "Epoch 500/500\n",
      "142/142 [==============================] - 3s 19ms/step - loss: 0.0828 - accuracy: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea947280a0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length = max_sequence_len - 1),\n",
    "    # Setting a unidirectional LSTM causes some repeatition, Bidirectional causes less\n",
    "    # tf.keras.layers.LSTM(20),\n",
    "    \n",
    "    # Instead, use a bidirectional LSTM\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # One neuron per word\n",
    "    tf.keras.layers.Dense(total_words, \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "optimizer = tf.keras.optimizers.Adam(),\n",
    "metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(xs, ys, epochs = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  46   7  55   3 192 101  22\n",
      "   24]]\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Harry was not a normal boy at all\"\n",
    "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen= max_sequence_len - 1, padding = \"pre\")\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(token_list)\n",
    "prediction = np.argmax(prediction)\n",
    "print(tokenizer.index_word[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Harry potter is never seen to see the air and landed the letter it black hair and beard hid most of a cat put landed landed landed arrived last wearing beard hid door where he changed of the next lamp too was as or are even still strange dore and at are even\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Harry potter is\"\n",
    "\n",
    "for i in range(50):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen= max_sequence_len - 1, padding = \"pre\")\n",
    "    prediction = model.predict(token_list)\n",
    "    prediction = np.argmax(prediction)\n",
    "    prediction = tokenizer.index_word[prediction]\n",
    "    seed_text += \" \" + prediction\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancements\n",
    "- larger data sets\n",
    "- increase embedding dim\n",
    "- increase # LSTM units (16 ➡️ 150)\n",
    "- Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
