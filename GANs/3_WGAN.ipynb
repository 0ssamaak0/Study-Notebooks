{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I5bBgU_wfhNO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1- WGPAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vIcj5eyIfkVF"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super().__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # Input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # 32 x 32\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # no batch norm on first layer\n",
        "            self._block(features_d, features_d*2, 4, 2, 1), # (32x32 -> 16x16)\n",
        "            self._block(features_d*2, features_d*4, 4, 2, 1), # (16x16 -> 8x8)\n",
        "            self._block(features_d*4, features_d*8, 4, 2, 1), # (8x8 -> 4x4)\n",
        "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # (4x4 -> 1x1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False, # batch norm handles bias\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uy_XOeEcflHg"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, channels_img, features_g):\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            # Input: N x z_dim x 1 x 1\n",
        "            self._block(z_dim, features_g*16, 4, 1, 0), # (1x1 -> 4x4)\n",
        "            self._block(features_g*16, features_g*8, 4, 2, 1), # (4x4 -> 8x8)\n",
        "            self._block(features_g*8, features_g*4, 4, 2, 1), # (8x8 -> 16x16)\n",
        "            self._block(features_g*4, features_g*2, 4, 2, 1), # (16x16 -> 32x32)\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g*2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ), # (32x32 -> 64x64)\n",
        "            # no batch norm on last layer\n",
        "            nn.Tanh(), # [-1, 1]\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False, # batch norm handles bias\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(), # no leaky relu (same as paper)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "idcDILnefmHa"
      },
      "outputs": [],
      "source": [
        "# Initialize weights (taken from DCGAN paper)\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "# testing discriminator and generator\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    z_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Critic(in_channels, 8)\n",
        "    initialize_weights(disc)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(z_dim, in_channels, 8)\n",
        "    initialize_weights(gen)\n",
        "    z = torch.randn((N, z_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jROI5GO1fm7W"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 5e-5 # from DCGAN paper\n",
        "batch_size = 64\n",
        "image_size = 64 # from DCGAN paper\n",
        "channels_img = 1\n",
        "z_dim = 100\n",
        "features_d = 64\n",
        "features_g = 64\n",
        "num_epochs = 5\n",
        "critic_iterations = 5\n",
        "weight_clip = 0.01\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]\n",
        "            # general for any number of channels\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xgzFbtEpjQW5"
      },
      "outputs": [],
      "source": [
        "# prepare dataset\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HGT7dA0Njey1"
      },
      "outputs": [],
      "source": [
        "# define networks\n",
        "gen = Generator(z_dim, channels_img, features_g).to(device)\n",
        "critic = Critic(channels_img, features_d).to(device)\n",
        "\n",
        "# initialize weights\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# optimizers\n",
        "opt_gen = optim.RMSprop(gen.parameters(), lr=learning_rate)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "# set to train mode\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "# fixed noise\n",
        "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
        "\n",
        "# tensorboard writers\n",
        "writer_real = SummaryWriter(f\"logs/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "step = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G79eJDkvkEb2",
        "outputId": "ed017411-7380-4f15-e63f-ade702251e06"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (real, _) in enumerate(tqdm(loader)):\n",
        "        real = real.to(device) # real images\n",
        "\n",
        "\n",
        "        for _ in range(critic_iterations):\n",
        "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
        "            fake = gen(noise) # fake images\n",
        "\n",
        "            critic_real = critic(real).view(-1) # real images\n",
        "            critic_fake = critic(fake).view(-1) # fake images\n",
        "            \n",
        "            loss_critic = -(torch.mean(critic_fake) - torch.mean(critic_real)) # Maximize loss\n",
        "\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "            # Clip weights\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-weight_clip, weight_clip)\n",
        "\n",
        "        # Train Generator: min -E[critic(gen_fake)]\n",
        "        output = critic(fake).view(-1)\n",
        "        lossG = -torch.mean(output)\n",
        "        # Backpropagation (generator)\n",
        "        gen.zero_grad()\n",
        "        lossG.backward()\n",
        "        # Update weights\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch: [{epoch}/{num_epochs}] \\ \"\n",
        "                f\"LossD: {loss_critic:.8f}, LossG: {lossG:.8f}\"\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                data = real\n",
        "\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "\n",
        "                # write to tensorboard\n",
        "                # writer fake images\n",
        "                writer_fake.add_image(\n",
        "                    \"Fake Images\", img_grid_fake, global_step=step\n",
        "                )\n",
        "                # writer real images\n",
        "                writer_real.add_image(\n",
        "                    \"Real Images\", img_grid_real, global_step=step\n",
        "                )\n",
        "                step += 1\n",
        "\n",
        "                plt.imshow(img_grid_fake.to(\"cpu\").permute(1, 2, 0))\n",
        "                plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2- WGAN-GP \n",
        "WGAN-GP is a variant of WGAN that uses gradient penalty to enforce the Lipschitz constraint instead of weight clipping. \n",
        "\n",
        "weight clipping is a bad way to enforce Lipschitz constraint because it can lead to undesired behavior such as vanishing gradients or exploding gradients. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gradient penalty function\n",
        "def gradient_penatly(critic, real, fake, device = \"cpu\"):\n",
        "    # get batch size, channels, height, width\n",
        "    batch_size, C, H, W = real.shape\n",
        "    # generate random epsilon (0, 1)\n",
        "    epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    # interpolate between real and fake images\n",
        "    interpolated_images = real * epsilon + fake * (1 - epsilon)\n",
        "    # calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "    # gradient of scores wrt interpolated images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    # flatten gradient\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    # calculate norm of gradient\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    # gradient penalty\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Instance Norm instead of Batch Norm in the Critic\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super().__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # Input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # 32 x 32\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # no batch norm on first layer\n",
        "            self._block(features_d, features_d*2, 4, 2, 1), # (32x32 -> 16x16)\n",
        "            self._block(features_d*2, features_d*4, 4, 2, 1), # (16x16 -> 8x8)\n",
        "            self._block(features_d*4, features_d*8, 4, 2, 1), # (8x8 -> 4x4)\n",
        "            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # (4x4 -> 1x1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False, # batch norm handles bias\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 1e-4 # from DCGAN paper\n",
        "batch_size = 64\n",
        "image_size = 64 # from DCGAN paper\n",
        "channels_img = 1\n",
        "z_dim = 100\n",
        "features_d = 64\n",
        "features_g = 64\n",
        "num_epochs = 5\n",
        "critic_iterations = 5\n",
        "lambda_gp = 10\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]\n",
        "            # general for any number of channels\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define networks\n",
        "gen = Generator(z_dim, channels_img, features_g).to(device)\n",
        "critic = Critic(channels_img, features_d).to(device)\n",
        "\n",
        "# initialize weights\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# optimizers\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=learning_rate, betas = (0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=learning_rate, betas = (0.0, 0.9))\n",
        "\n",
        "# set to train mode\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "# fixed noise\n",
        "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
        "\n",
        "# tensorboard writers\n",
        "writer_real = SummaryWriter(f\"logs/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "step = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (real, _) in enumerate(tqdm(loader)):\n",
        "        real = real.to(device) # real images\n",
        "\n",
        "\n",
        "        for _ in range(critic_iterations):\n",
        "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
        "            fake = gen(noise) # fake images\n",
        "\n",
        "            critic_real = critic(real).view(-1) # real images\n",
        "            critic_fake = critic(fake).view(-1) # fake images\n",
        "            \n",
        "            # gradient penalty\n",
        "            gp = gradient_penatly(critic, real, fake, device=device)\n",
        "            \n",
        "            loss_critic = - (torch.mean(critic_real) - torch.mean(critic_fake)) + lambda_gp * gp # Maximize loss\n",
        "\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "            # Clip weights\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-weight_clip, weight_clip)\n",
        "\n",
        "        # Train Generator: min -E[critic(gen_fake)]\n",
        "        output = critic(fake).view(-1)\n",
        "        lossG = -torch.mean(output)\n",
        "        # Backpropagation (generator)\n",
        "        gen.zero_grad()\n",
        "        lossG.backward()\n",
        "        # Update weights\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch: [{epoch}/{num_epochs}] \\ \"\n",
        "                f\"LossD: {loss_critic:.8f}, LossG: {lossG:.8f}\"\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                data = real\n",
        "\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "\n",
        "                # write to tensorboard\n",
        "                # writer fake images\n",
        "                writer_fake.add_image(\n",
        "                    \"Fake Images\", img_grid_fake, global_step=step\n",
        "                )\n",
        "                # writer real images\n",
        "                writer_real.add_image(\n",
        "                    \"Real Images\", img_grid_real, global_step=step\n",
        "                )\n",
        "                step += 1\n",
        "\n",
        "                plt.imshow(img_grid_fake.to(\"cpu\").permute(1, 2, 0))\n",
        "                plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
